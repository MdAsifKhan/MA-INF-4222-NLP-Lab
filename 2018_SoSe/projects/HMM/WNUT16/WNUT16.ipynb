{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "# from hmm_class.hmmd_scaled import HMM\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=True, strip_handles=False, reduce_len=False)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for getting the word and NER tag pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples(dspath):\n",
    "    sentences = []\n",
    "    s = ''\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    poss = []\n",
    "    tot_sentences = 0\n",
    "    ners_by_position = []\n",
    "    index = 0\n",
    "    with open(dspath) as f:\n",
    "        for line in f:\n",
    "            if line.strip() != '':\n",
    "                token = line.split('\\t')[0].decode('utf-8')\n",
    "                ner = line.split('\\t')[1].replace('\\r', '').replace('\\n', '').decode('utf-8')\n",
    "                '''\n",
    "                if ner in definitions.NER_TAGS_ORG:\n",
    "                    ner = 'ORG'\n",
    "                elif ner in definitions.NER_TAGS_LOC:\n",
    "                    ner = 'LOC'\n",
    "                elif ner in definitions.NER_TAGS_PER:\n",
    "                    ner = 'PER'\n",
    "                else :\n",
    "                    ner = 'O'\n",
    "                '''\n",
    "                #ners_by_position.append([index, len(token), ner])\n",
    "                index += len(token) + 1\n",
    "            if line.strip() == '':\n",
    "                if len(tokens) != 0:\n",
    "                    #poss = [x[1].decode('utf-8') for x in nltk.pos_tag(nltk.word_tokenize(s[:-1]))]\n",
    "                    poss = [x[1].decode('utf-8') for x in nltk.pos_tag(tknzr.tokenize(s[:-1]))]\n",
    "\n",
    "\n",
    "                    #if len(poss) == len(tokens): # tokenization doesnt affect position of NERs, i.e., same tokenization\n",
    "                    sentences.append(zip(tokens, poss, ners))\n",
    "                    #else:\n",
    "                    #    aux = 0\n",
    "                    #    for i in range(len()):\n",
    "                    #        if aux <= tokens[i]\n",
    "\n",
    "                    tokens = []\n",
    "                    ners = []\n",
    "                    s = ''\n",
    "                    tot_sentences += 1\n",
    "            else:\n",
    "                s += token + ' '\n",
    "                tokens.append(token)\n",
    "                ners.append(ner)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to creating the feature vectors from the wrods and applying the classification on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1].encode(\"utf-8\")\n",
    "\n",
    "    #my addition\n",
    "    ner_class = sent[i][2].encode(\"utf-8\")\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2].encode(\"utf-8\"),\n",
    "        'stop_word': word in stop,\n",
    "        'hyphen': '-' in word,\n",
    "        'size_small': True if len(word) <= 2 else False,\n",
    "        'stemmer_lanc': lancaster_stemmer.stem(word),\n",
    "\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def word2features_new(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1].encode(\"utf-8\")\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2].encode(\"utf-8\"),\n",
    "        'stop_word': word in stop,\n",
    "        'hyphen': '-' in word,\n",
    "        'size_small': True if len(word) <= 2 else False,\n",
    "        'stemmer_lanc': lancaster_stemmer.stem(word),\n",
    "        'klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "        'klass': tf_idf_clone.predict([word])[0],\n",
    "        'klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "        'klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '-1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '-1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '-1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '-1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '+1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '+1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '+1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '+1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def remove_extra_features(X_train_new, X_test_new):\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    for sentence in X_train_new:\n",
    "        tmp = []\n",
    "        for token in sentence:\n",
    "            del token['klass']\n",
    "            del token['klass_1']\n",
    "            del token['klass_2']\n",
    "            del token['klass_3']\n",
    "\n",
    "            if '+1:klass' in token:\n",
    "                del token['+1:klass']\n",
    "                del token['+1:klass_1']\n",
    "                del token['+1:klass_2']\n",
    "                del token['+1:klass_3']\n",
    "\n",
    "            if '-1:klass' in token:\n",
    "                del token['-1:klass']\n",
    "                del token['-1:klass_1']\n",
    "                del token['-1:klass_2']\n",
    "                del token['-1:klass_3']\n",
    "            tmp.append(token)\n",
    "            X_train.append(tmp)\n",
    "\n",
    "    for sentence in X_test_new:\n",
    "            tmp = []\n",
    "            for token in sentence:\n",
    "\n",
    "                del token['klass']\n",
    "                del token['klass_1']\n",
    "                del token['klass_2']\n",
    "                del token['klass_3']\n",
    "\n",
    "                if '+1:klass' in token:\n",
    "                    del token['+1:klass']\n",
    "                    del token['+1:klass_1']\n",
    "                    del token['+1:klass_2']\n",
    "                    del token['+1:klass_3']\n",
    "\n",
    "                if '-1:klass' in token:\n",
    "                    del token['-1:klass']\n",
    "                    del token['-1:klass_1']\n",
    "                    del token['-1:klass_2']\n",
    "                    del token['-1:klass_3']\n",
    "                tmp.append(token)\n",
    "                X_test.append(tmp)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def group_labels(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok.find(\"geo-loc\") != -1 or tok.find(\"location\") != -1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok.find(\"company\") != -1 or tok.find(\"corporation\") != -1:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok.find(\"person\") != -1:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def group_labels_num(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok == 1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok == 2:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok == 3:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features_new(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label.encode(\"utf-8\") for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "#print(\"i am here1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the before created dataset (saves time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_clone_1 = joblib.load('tf-idf+svm_1.pkl')\n",
    "tf_idf_clone_2 = joblib.load('tf-idf+svm_2.pkl')\n",
    "tf_idf_clone_3 = joblib.load('tf-idf+svm_3.pkl')\n",
    "tf_idf_clone = joblib.load('tf-idf+svm_new.pkl')\n",
    "\n",
    "\n",
    "\n",
    "X_train = joblib.load('X_train.pkl')\n",
    "X_train_new = joblib.load('X_train_new.pkl')\n",
    "y_train = joblib.load('y_train.pkl')\n",
    "\n",
    "X_test = joblib.load('X_test.pkl')\n",
    "X_test_new = joblib.load('X_test_new.pkl')\n",
    "y_test = joblib.load('y_test.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HMM part - First getting the frequencies of the different entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, labels, test,testy, split_sequences=False):\n",
    "    # if not os.path.exists('chunking'):\n",
    "    #     print(\"Please create a folder in your local directory called 'chunking'\")\n",
    "    #     print(\"train.txt and test.txt should be stored in there.\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/train.txt'):\n",
    "    #     print(\"train.txt is not in chunking/train.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/test.txt'):\n",
    "    #     print(\"test.txt is not in chunking/test.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "\n",
    "    word2idx = {}\n",
    "    tag2idx = {}\n",
    "    word_idx = 0\n",
    "    tag_idx = 0\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    \n",
    "\n",
    "    for line1,line2 in itertools.izip(train,labels):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #print(\"class\")\n",
    "                #r = line.split()\n",
    "\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2\n",
    "            #print(tag)\n",
    "            #word, tag = r\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            currentX.append(word2idx[word])\n",
    "\n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtrain.append(currentX)\n",
    "            Ytrain.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "\n",
    "    # load and score test data\n",
    "    Xtest = []\n",
    "    Ytest = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line1,line2 in itertools.izip(test,testy):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #r = line.split()\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2#[:2]\n",
    "            #word, tag, = r\n",
    "            if word in word2idx:\n",
    "                currentX.append(word2idx[word])\n",
    "            else:\n",
    "                currentX.append(word_idx)  # use this as unknown\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtest.append(currentX)\n",
    "            Ytest.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx\n",
    "\n",
    "def random_normalized(d1, d2):\n",
    "    x = np.random.random((d1, d2))\n",
    "    return x / x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM model creation - train the HMM, find the hidden state matrix, create observation matrix, Baum-Welch and Viterbi used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, M):\n",
    "        self.M = M  # number of hidden states\n",
    "\n",
    "    def fit(self, X, max_iter=30):\n",
    "        np.random.seed(123)\n",
    "        # train the HMM model using the Baum-Welch algorithm\n",
    "        # a specific instance of the expectation-maximization algorithm\n",
    "\n",
    "        # determine V, the vocabulary size\n",
    "        # assume observables are already integers from 0..V-1\n",
    "        # X is a jagged array of observed sequences\n",
    "        V = max(max(x) for x in X) + 1\n",
    "        N = len(X)\n",
    "\n",
    "        self.pi = np.ones(self.M) / self.M  # initial state distribution\n",
    "        self.A = random_normalized(self.M, self.M)  # state transition matrix\n",
    "        self.B = random_normalized(self.M, V)  # output distribution\n",
    "\n",
    "        print(\"initial A:\", self.A)\n",
    "        print(\"initial B:\", self.B)\n",
    "\n",
    "        costs = []\n",
    "        for it in range(max_iter):\n",
    "            if it % 10 == 0:\n",
    "                print(\"it:\", it)\n",
    "            # alpha1 = np.zeros((N, self.M))\n",
    "            alphas = []\n",
    "            betas = []\n",
    "            scales = []\n",
    "            logP = np.zeros(N)\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                scale = np.zeros(T)\n",
    "                # alpha1[n] = self.pi*self.B[:,x[0]]\n",
    "                alpha = np.zeros((T, self.M))\n",
    "                alpha[0] = self.pi * self.B[:, x[0]]\n",
    "                scale[0] = alpha[0].sum()\n",
    "                alpha[0] /= scale[0]\n",
    "                for t in range(1, T):\n",
    "                    alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "                    scale[t] = alpha_t_prime.sum()\n",
    "                    alpha[t] = alpha_t_prime / scale[t]\n",
    "                logP[n] = np.log(scale).sum()\n",
    "                alphas.append(alpha)\n",
    "                scales.append(scale)\n",
    "\n",
    "                beta = np.zeros((T, self.M))\n",
    "                beta[-1] = 1\n",
    "                for t in range(T - 2, -1, -1):\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t + 1]] * beta[t + 1]) / scale[t + 1]\n",
    "                betas.append(beta)\n",
    "\n",
    "            cost = np.sum(logP)\n",
    "            costs.append(cost)\n",
    "\n",
    "            # now re-estimate pi, A, B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0]) for n in range(N)) / N\n",
    "\n",
    "            den1 = np.zeros((self.M, 1))\n",
    "            den2 = np.zeros((self.M, 1))\n",
    "            a_num = np.zeros((self.M, self.M))\n",
    "            b_num = np.zeros((self.M, V))\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                den1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T\n",
    "                den2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T\n",
    "\n",
    "                # numerator for A\n",
    "                # a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T - 1):\n",
    "                            a_num[i, j] += alphas[n][t, i] * betas[n][t + 1, j] * self.A[i, j] * self.B[j, x[t + 1]] / \\\n",
    "                                           scales[n][t + 1]\n",
    "                # a_num += a_num_n\n",
    "\n",
    "                # numerator for B\n",
    "                # for i in range(self.M):\n",
    "                #     for j in range(V):\n",
    "                #         for t in range(T):\n",
    "                #             if x[t] == j:\n",
    "                #                 b_num[i,j] += alphas[n][t][i] * betas[n][t][i]\n",
    "                for i in range(self.M):\n",
    "                    for t in range(T):\n",
    "                        b_num[i, x[t]] += alphas[n][t, i] * betas[n][t, i]\n",
    "            self.A = a_num / den1\n",
    "            self.B = b_num / den2\n",
    "        print(\"A:\", self.A)\n",
    "        print(\"B:\", self.B)\n",
    "        print(\"pi:\", self.pi)\n",
    "\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        # returns log P(x | model)\n",
    "        # using the forward part of the forward-backward algorithm\n",
    "        T = len(x)\n",
    "        scale = np.zeros(T)\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi * self.B[:, x[0]]\n",
    "        scale[0] = alpha[0].sum()\n",
    "        alpha[0] /= scale[0]\n",
    "        for t in range(1, T):\n",
    "            alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "            scale[t] = alpha_t_prime.sum()\n",
    "            alpha[t] = alpha_t_prime / scale[t]\n",
    "        return np.log(scale).sum()\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        return np.array([self.log_likelihood(x) for x in X])\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        # returns the most likely state sequence given observed sequence x\n",
    "        # using the Viterbi algorithm\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        #print(x)\n",
    "        delta[0] = np.log(self.pi) + np.log(self.B[:, x[0]])\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.M):\n",
    "                delta[t, j] = np.max(delta[t - 1] + np.log(self.A[:, j])) + np.log(self.B[j, x[t]])\n",
    "                psi[t, j] = np.argmax(delta[t - 1] + np.log(self.A[:, j]))\n",
    "\n",
    "        # backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T - 1] = np.argmax(delta[T - 1])\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            states[t] = psi[t + 1, states[t + 1]]\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics - Accuracy, F1-Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for t, y in zip(T, Y):\n",
    "        #print(\"miamano\")\n",
    "        #print(t,y)\n",
    "        for x, z in zip(t, y):\n",
    "            if x != 0:\n",
    "                if x == z:\n",
    "                    n_correct += 1\n",
    "                n_total += 1\n",
    "        #n_correct += np.sum(t == y)\n",
    "        #print(\"The sum\", np.sum(t == y))\n",
    "        #n_total += len(y)\n",
    "    return float(n_correct) / n_total\n",
    "\n",
    "\n",
    "def total_f1_score(T, Y, labels):\n",
    "    # inputs are lists of lists\n",
    "    \n",
    "\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    #print(\"Try to remove: \", labels)\n",
    "    labels.discard(0)\n",
    "    return f1_score(T, Y, labels=list(labels), average=None).mean()\n",
    "\n",
    "def total_conf_matrix(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    return confusion_matrix(T, Y)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HMM class and getting the prediction and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test accuracy:', 0.3224990971469845)\n",
      "('test f1:', 0.01489244875951023)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAEYCAYAAAAnEYFiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8FPXdwPHPN1kuFUgQEJJwBIKEBJEjgIKKJ4JcHiBYURGf0lZF0dZWpUWLtqjYoj7Sx4cWK1IriMcTQOSQFhUqhsMDOdQgUJKoCHJokWCW7/PHbsJmc+zCZjO7k++7r3m5s/Obme+P2X7z+83Mb0ZUFWOMcasEpwMwxphosiRnjHE1S3LGGFezJGeMcTVLcsYYV7MkZ4xxNUtyxhhXsyRnjHE1S3LGGFezJGfKEZGdIvILEflIRA6KyHwRaehfNkJEPhCRQyKyXUQG+b9fJSIPi8i/ROQ7EVkkIqeLyAv+sutEpL2T9TJ1l9iwLhNIRHYCe4ArgSPAGuBJYCOwAhgJrARaA41VdZuIrALSgMuBvcC7gAe4FVgFPAt4VfXmWqyKMYDvh2hMsKdUtQhARBYB3YEewLOqusJfpjBonb+q6nb/Om8AWar6pn9+AfBQrURuTBDrrprKfBnw+TBwGtAG2F7NOl8FfP6+kvnTaiw6Y06AJTkTrt1AR6eDMOZEWZIz4ZoN3Cwil4hIgoikikim00EZE4olORMWVc0DbgZmAAeBt4B2jgZlTBjs6qoxxtWsJWeMcTVLcsYYV7MkZ4xxNUtyxhhXi6kRD6c3b67t2rV3OowaJ04HEEXvbytwOoSo6JGZ5nQIUbFr10727t1boz/JxCbtVEu+D6usfv/1MlUdVJP7DyWmkly7du15a02e02HUuHoe9zaYk/vf43QIUbFmzXSnQ4iK/n1zanybWvI9DTpfG1bZIx/MbF7jAYQQU0nOGBOPBCR2/5BbkjPGREaAhESno6iSJTljTOQkds88W5IzxkTIuqvGGLezlpwxxrUEa8kZY9xMrCVnjHE5u7pqjHEvu/BgjHEzwbqrxhiXs5acMca9rLtqjHG7BOuuGmPcysauGmPczbqrxhi3s6urxhhXs5acMca1JLaHdcVu+j1JK5YvpWe3LpydfSZ/nP5oheXFxcWMGzuGs7PP5KLzz2XXrp0ArF+XR/++Penftyf9+vRgUe5rtRx5aMuXLaVbdmeyMzOY/tgjFZYXFxcz9kejyc7M4Px+fdm1c2fZsumPTiM7M4Nu2Z1ZsXxZLUYd2mXndObDl+7h45d/xS9uvKjC8ratkljy9ATy/nY3y/70U1JbNgXggl4dWTv3rrJp/9u/Z9gF2bUdfrXceswqSEgMb3IiNEf2GiVer5efT5rIK7mvs+79j3l5wTy2bd1Srszzzz1LUnIyH27+lNsm3skDk+8FICu7K2+tyWPNext5NXcJd078GSUlJU5Uo1Jer5dJd9xG7qI3eP+jLSyY9yJbt5Sv23PPziY5KZnN2/KZeOddTL7/VwBs3bKFBfPnsfHDzSxcvJQ7J96K1+t1ohoVJCQIT9xzFSMmzabHmMcZNbA7mekty5WZdsdQXliygT5j/8jvZ69g6q2DAXh7w3bOuWEG59wwg8G3PcPhIz/w5nufOlGNSrn1mFXkv/AQzuQAVyW59evy6NCxI+npHahfvz7XjBrN64sXlivz+uJcrrv+RgCuvHokq1b9A1XllFNOwePx9d6PFB9BYqz5vS4vj44dM0jv4KvbqNFjWLwot1yZxYtyuf6GmwC4+pqRrPrHSlSVxYtyGTV6DA0aNKB9ejodO2awLi82XhjUO6st2wv2srPoG34o8bJgxQcMDWqNZaafwar1+QC8tWF7heUAV13cjeXvbuP74h9qJe5wuPWYVaq0yxpqcoCrktwXRYWkpbUpm09JTaWosDCoTFFZGY/HQ5MmTflm3z4A1uW9R5+eZ3Fuztk88dSfypJeLCgKqltqahqFQXUrKiokrU1A3Zo2Zd++fRQWVly3qKj8uk5JadmEgq8OlM0X7jlIaoum5cps+uwLrrzoLABGXNiVJqc2pFmTU8qVGXVZd15a/kH0Az4Bbj1mFZQ+T66uteRE5FkR2SMiH0drH8FUtbI4QpYp/QvTu09f8jZuYtXq9/jD9Ec5cuRIVOI8GSdbNxGBMNZ1ilTyVtrgetz31GLO79GBd5+fxPk9O1C45wAl3mNly1ud3pjsjq1YsfaTqMd7Itx6zCqqu93V54BafYlsSmoaBQW7y+aLCgtpnZISVCa1rExJSQmHDh2kWbNm5cp0zuzCqaeeypbNtZafQ0oNqlthYQEpQXVLTU2jYHdA3Q766paaVnHd1q3Lr+uUwj0HSTsjqWw+tWVTivYeKlfmi72HGHPv85x74xM88D9LATj0n+N/gK659GwWvvVxucQXC9x6zCpVF7urqvo28E20tl+ZXjm9+Tw/n507d3D06FFeWTCfK4YMK1fmiiHDefGF5wH4v1dfZsCAixARdu7cUXah4d+7dvHZp5/Qrl372gy/Wjm9e5Of/xk7d/jqtmD+PIYMHV6uzJChw3lh7hwAXn3lZQZcdDEiwpChw1kwfx7FxcXs3LGD/PzP6N2njxPVqGD91t1ktGlOu9bJ1PMkMuqy7rz+dvmT86c3PaWsFXPPTRczZ9G6csuvHRh7XVVw7zGrVAxfXY2dk041wOPxMH3GU1w1bDBer5cbbrqZLlnZPDz1AXr27MUVQ4dz47jxTBh/I2dnn0lycjP+OvfvALz7r9XMePwx6tWrR0JCAn988mlOb17rL/uuksfjYcaTTzNsyOV4vV5uGjeerOxspj44hZ69chg6bDjjxt/C+HE3kJ2ZQXJyM+a+MA+ArOxsrhl1LT26ZeHxeHjiqZkkJsbGWEOv9xh3Pf5/LHrqxyQmJDBnUR5bd3zFbyYMZOPWAl5/ZwsX9OrI1FsHowqr3/+cSdOP397TtnUyaS2TeGfj5w7WonJuPWYVSGwP65JKz1HV1MZF2gOLVbVrNWUmABMA2rRp22vzpzuiFo9T6nli9wcQqeT+9zgdQlTsXzPd6RCion/fHDZsWF+j/caE5Pba4KLfhFX2yGv/tUFVc2py/6E4/v8+VZ2lqjmqmtO8RQunwzHGnAQRCWtygqu6q8aY2ud7+nmsXvmN7i0kLwLvAp1FpEBEbonWvowxDpITmBwQtZacql4XrW0bY2KJkJDg+JmvKll31RgTsVjurlqSM8ZEzJKcMca9HDzfFg5LcsaYiAjO3R4SDktyxpiI2YUHY4yrWUvOGONedk7OGON2sdySi92OtDEmLpReeKiJsasiMkhEPhGRfBG5t5LlbUXknyLyvoh8JCJXhNqmJTljTMRqIsmJSCIwExgMZAHXiUhWULFfAy+pag9gDPCnULFZkjPGREZAEiSsKYQ+QL6qfq6qR4F5wIigMgo08X9uChSF2qidkzPGROwEzsk1F5H1AfOzVHWW/3MqsDtgWQHQN2j9B4HlIjIROBW4NNQOLckZYyJ2AklubzUPzaxsI8FP9b0OeE5V/yAi5wJzRaSrqlb5gg9LcsaYiNTgiIcCoE3AfBoVu6O34H9Blqq+KyINgebAnqo2aufkjDGRq5nnya0DOolIuojUx3dhYWFQmX8DlwCISBegIfB1dRu1lpwxJjJSM/fJqWqJiNwOLAMSgWdVdbOITAXWq+pC4OfAn0XkLnxd2XEa4kU1luSMMRGrqbGrqroEWBL03ZSAz1uA/ieyTUtyxpjIxe6AB0tyxpjIxfKwLktyxpiIOPm6wXBYkjPGRMySXJgEd75t/tixai/+xLcfjjgdgYkBluSMMa4WxrhUx1iSM8ZEpobuk4sWS3LGmIgIEMM5zpKcMSZSdnXVGONyMZzjLMkZYyIkkGAXHowxbiVYkjPGuJx1V40xrmYXHowx7iXWkjPGuJjvPrnYzXKW5IwxERK78GCMcTdryRlj3MvOyRlj3MzOyRljXC+Gc5wlOWNM5KwlZ4xxLxu7aoxxM3uenDHG5WL7eXKue2vM8mVL6ZbdmezMDKY/9kiF5cXFxYz90WiyMzM4v19fdu3cWbZs+qPTyM7MoFt2Z1YsX1aLUYdn+bKldO+ayVldOvH49MrrduP1YzirSycGnHdOWd327dvH4IEX07JZY+6+8/Zajjq0y/p14cNXf83HuVP4xbjLKixv2zqZJc/cTt78e1k26w5SWyaVLXv4juGsf+k+1r90HyMH9qzNsMPi5t9jIJHwJie4Ksl5vV4m3XEbuYve4P2PtrBg3ots3bKlXJnnnp1NclIym7flM/HOu5h8/68A2LplCwvmz2Pjh5tZuHgpd068Fa/X60Q1KuX1ern7ztt5beESNny4mQXz57F1a/m6zfnrbJKSkti09TNuv2MSv5l8LwANGzbkNw9M5fePTHci9GolJAhP/GoUIyb+Dz2u+R2jBvUiM71VuTLTJl3FC4vz6DP6EX7/56VMnTgMgEHnZdM9sw19r3uUC278A5NuvITGpzZ0ohqVcvPvMVjpu1dDTU5wVZJbl5dHx44ZpHfoQP369Rk1egyLF+WWK7N4US7X33ATAFdfM5JV/1iJqrJ4US6jRo+hQYMGtE9Pp2PHDNbl5TlRjUqtX5dHh4C6jbx2dCV1W1hWt6uuHsmqf/rqduqpp9Kv/3k0aBg7CaBU767t2F6wl52F+/ihxMuCZRsYeuFZ5cpkdmjFqrxPAXhr3acMHeBb3qVDK97ZkI/Xe4zDR46y6dNCBvbrUut1qIqbf4+BxH/hIZzJCa5KckVFhaSltSmbT01No7CwsGKZNr4yHo+HJk2bsm/fPgoLK65bVFR+XSf54k4rm09NTeOLyuqWFlC3Jr66xbKUFkkUfLm/bL5wz4Fy3VGATZ8WcuUlZwMw4uKzaXJaI5o1PYWPPi3k8v5ZNGpYj9OTTmVATifSzkiu1fir4+bfY7BYbslF7cKDiLQBngdaAceAWar6ZLT2B6Ba8SXOwf+wVZYJY10nhVO3WK9DZSoLL7iu9814jRn3XsvYYX1Zs3E7hV/tp8R7jJVrt9Eruy3//Ovd7N3/He99tIOSGOrSufn3GCyGQ4vq1dUS4OequlFEGgMbRGSFqm4JteLJSk1No6Bgd9l8YWEBKSkpFcvs3k1aWholJSUcOniQZs2akZpWcd3Wrcuv6yRf3AVl84WFBbQKqluKv/6ppXU75KtbLCvcc4C0VsdbX6ktkyj6+mC5Ml/sPcSYX/wFgFMb1efKS87m0HdHAHhs9nIem70cgOd+dxP5//66liIPzc2/x2CxnICj1l1V1S9UdaP/87fAViA1WvsDyOndm/z8z9i5YwdHjx5lwfx5DBk6vFyZIUOH88LcOQC8+srLDLjoYkSEIUOHs2D+PIqLi9m5Ywf5+Z/Ru0+faIZ7Qnrl9GZ7QN1efml+JXUbVla31159mQEXXhzTPz6A9Zv/TUabFrRLOZ16nkRGXd6L19/aVK7M6UmnltXjnvEDmZO7FvCdB2rW9BQAunZKoWunFN5cu612K1ANN/8eywnzyqpTP8VauU9ORNoDPYD3Klk2AZgA0KZt24j24/F4mPHk0wwbcjler5ebxo0nKzubqQ9OoWevHIYOG8648bcwftwNZGdmkJzcjLkvzAMgKzuba0ZdS49uWXg8Hp54aiaJiYkRxVOTPB4Pf3jivxkxdBBer5cbx91MVlY2D/12Cj175jBk2HBuuvkW/uvmGzmrSyeSmzVjztwXy9bvcmY63x46xNGjR1m0KJeFry+jS5csB2vk4/Ue465HF7Bo5q0kJghzFq5l6+df8pufXsHGLf/m9bc/5oJenZg6cRiqsHpjPpMeWQBAPU8ib86eBMC3/znC+F8/j9d7zMnqlOPm32MgifH75KSycwI1ugOR04C3gN+p6qvVle3VK0fXvLc+qvE44dix6P4bO+n0c+5wOoSo2J/3306HEBX9++awYcP6Gs1ITdp20b6//GtYZd+ceO4GVc2pyf2HEtWrqyJSD3gFeCFUgjPGxK+a6q6KyCAR+URE8kXk3irKXCsiW0Rks4j8PdQ2o3l1VYDZwFZV/WO09mOMcZYvgUXeOBSRRGAmcBlQAKwTkYWBFytFpBNwH9BfVfeLSMtQ260yyYlIk+pWVNVDIbbdH7gB2CQiH/i/u19Vl4QKyhgTX2roPt8+QL6qfg4gIvOAEUDgHRk/Bmaq6n4AVd0TaqPVteQ2A4rvIQOlSucVqPYqgaquDlrXGONSJ9CSay4igSfeZ6nqLP/nVGB3wLICoG/Q+mf697cGSAQeVNWl1e2wyiSnqm2qWmaMMYFOoLe6t5oLD5VtJfiqnQfoBFwIpAHviEhXVT1Q1Q7DuvAgImNE5H7/5zQR6RXOesYY9xMgUSSsKYQCILBxlQYUVVImV1V/UNUdwCf4kl6VQiY5EXkauAjf+TWAw8AzodYzxtQRYY5bDaNLuw7oJCLpIlIfGAMsDCrzf/jyESLSHF/39fPqNhrO1dV+qtpTRN4HUNVv/AEYYwxQM6MZVLVERG4HluE73/asqm4WkanAelVd6F82UES2AF7gHlWt9ikU4SS5H0QkAX/fWEROxzfg3hhjECChhkY8+O++WBL03ZSAzwrc7Z/CEs45uZn4buhtISK/BVYDj4a7A2OM+8X12FVVfV5ENgCX+r8apaofRzcsY0y8KH1oZqwKd8RDIvADvi6rqx60aYyJXE11V6MhnKurk4EXgRR8l3T/LiL3RTswY0z8kDAnJ4TTkhsL9FLVwwAi8jtgAzAtmoEZY+JHLD9qKZwktyuonIcQ96UYY+oO39VVp6OoWnUD9GfgOwd3GNgsIsv88wPxXWE1xpiym4FjVXUtudIrqJuB1wO+Xxu9cIwx8Sgur66q6uzaDMQYE5/itrtaSkQ6Ar8DsoCytxOr6plRjMsYE0diubsazj1vzwF/xZewBwMvAfOiGJMxJs7E8i0k4SS5U1R1GYCqblfVX+N/CoAxxoj4bgYOZ3JCOLeQFPvf17BdRH4KFAIhn6tujKk7Yri3GlaSuws4DbgD37m5psD4aAZljIkvcXl1tZSqlr4Q+luOPzjTGGMA38ulY3nsanU3A79Gxeerl1HVq6MSkTEmvjj4GKVwVNeSe7rWonC5WG7KR0yr/Dto6pBYvoWkupuBV9ZmIMaY+BXLz18L93lyxhhTKSFOW3LGGBMuTww35cJOciLSQFWLoxmMMSb++N7fELstuXCeDNxHRDYBn/nnzxaR/456ZMaYuJEg4U2OxBZGmaeAocA+AFX9EBvWZYwJENdv6wISVHVXUHPUG6V4jDFxpibfuxoN4SS53SLSB1ARSQQmAp9GNyxjTDxJjN0cF1aS+xm+Lmtb4CvgTf93xhiDOPiEkXCEM3Z1DzCmFmIxxsSpGM5xYT0Z+M9UMoZVVSdEJSJjTNyJ5ZGL4XRX3wz43BC4CtgdnXCMMfEm7i88qOr8wHkRmQusiFpExpi4E8M57qSGdaUD7Wo6EGNMnBJIjOEsF845uf0cPyeXAHwD3BvNoIwx8SOuX0nof7fD2fje6wBwTNUeIGaMKS+Wk1y1w7r8Ce01VfX6J0twxpgKRCSsyQnhjF3NE5GeUY/EGBOXSrurcTdAX0RKu7Ln4Ut0n4jIRhF5X0Q21k54xpiYF+bg/HAaciIyyJ9r8kWkynP/IjJSRFREckJts7qWXJ7/v1cCnYErgFHASP9/Y9LyZUvplt2Z7MwMpj/2SIXlxcXFjP3RaLIzMzi/X1927dxZtmz6o9PIzsygW3ZnVixfVotRh8etdXvmgevZtXIa6xfcX2WZP/xyJB/nPkDe/PvonplW9v31w/qyKXcKm3KncP2wvrUR7glx6zELJIAnQcKaqt2Ob2z8TGAwkAVcJyJZlZRrjO8Vqe8FL6tMdUlOAFR1e2VTOBuvbV6vl0l33Ebuojd4/6MtLJj3Ilu3bClX5rlnZ5OclMzmbflMvPMuJt//KwC2btnCgvnz2PjhZhYuXsqdE2/F642dh624uW5zF61lxG0zq1x++XlZdGzbgq4jfsvtD7/IU/f7RhkmNzmFyRMGc8ENj3P+2OlMnjCYpMaNaivskNx8zILVUEuuD5Cvqp+r6lFgHjCiknIPAY8BR8KJrbok10JE7q5qCmfjtW1dXh4dO2aQ3qED9evXZ9ToMSxelFuuzOJFuVx/w00AXH3NSFb9YyWqyuJFuYwaPYYGDRrQPj2djh0zWJeXV9luHOHmuq3ZuJ1vDh6ucvnQAd34+2JfvHmbdtK0cSNaNW/CZf26sHLtNvYfOsyBb79n5dptDOxf4Q+/Y9x8zMoTEsKcgOYisj5gChwemkr50VQF/u+O70mkB9BGVReHG111SS4ROA1oXMUUc4qKCklLa1M2n5qaRmFhYcUybXxlPB4PTZo2Zd++fRQWVly3qKj8uk5yc91CSWmZRMGX+8vmC786QErLJFJaJFHwVcD3ew6Q0iLJiRArVVeOme9FNmG35Paqak7ANCtoU8HK7ugQkQRgBvDzE4mvuvvkvlDVqSeysUAi0hB4G2jg38/LqvrAyW4vHJXd4RJ82brKMmGs6yQ31y2UykJV1cq/r/p96LWuzhyzmrtyWgC0CZhPA4oC5hsDXYFV/n+LVsBCERmuquur2mjIc3IRKAYuVtWzge7AIBE5J8JtVis1NY2CguOt3cLCAlJSUiqW2e0rU1JSwqGDB2nWrBmpaRXXbd26/LpOcnPdQin86gBprZLL5lPPSOKLrw9SuOcAaWcEfN/S932sqCvHTIDEBAlrCmEd0ElE0kWkPr5HvC0sXaiqB1W1uaq2V9X2wFqg2gQH1Se5S8KpYFXU5zv/bD3/FNU/szm9e5Of/xk7d+zg6NGjLJg/jyFDh5crM2TocF6YOweAV195mQEXXYyIMGTocBbMn0dxcTE7d+wgP/8zevfpE81wT4ib6xbK629t4kdDffH2Oas9h777ni/3HmLFv7Zy6bmZJDVuRFLjRlx6biYr/rXV4WiPq0vHLMH/4MxQU3VUtQS4HVgGbAVeUtXNIjJVRIZXu3I1quyuquo3J7vRUv5LwhuADGCmqoZ1yfdkeTweZjz5NMOGXI7X6+WmcePJys5m6oNT6Nkrh6HDhjNu/C2MH3cD2ZkZJCc3Y+4L8wDIys7mmlHX0qNbFh6PhyeemkliYmI0wz0hbq7bnGnjOL9XJ5onnUb+0od46Jkl1PP44vvLy6tZunozl5+XzeaFD3D4yA/85MG/AbD/0GGm/Xkpq//2SwB+P2sp+w9VfQGjtrn5mAWrqZ60qi4BlgR9N6WKsheGs02pjZFaIpIEvAZMVNWPg5ZNACYAtGnbtten23dFPR5Tc5J73+50CFGxf93TTocQFf375rBhw/oaPbmX3qWbPvB8eBc7b+7TboOqhryBtybVynuvVfUAsAoYVMmyWaVXWlo0b1Eb4RhjapLE/9jVkyIiLfwtOESkEXApsC1a+zPGOEfCnJxwMg/NDFdrYI7/vFwCvpOIYd/AZ4yJD0KcPzTzZKnqR0CPaG3fGBM7YjjHRbUlZ4ypE5w73xYOS3LGmIgItXQF8yRZkjPGRMxacsYYV4vdFGdJzhgTIYn3VxIaY0wo1l01xrha7KY4S3LGmBoQww05S3LGmMj4biGJ3SxnSc4YEzFryRljXCz0AzGdZEnOGBMR664aY9wtvHeqOsaSnDEmYpbkjDGuJtZdNca4VZ19aKYxpu6I4RxnSc4YEznrrhpjXEuAhNjNcZbkjDGREmvJGWNczO6TM8a4mV1dNca4XuymOEtyxpiaEMNZzpKcMSZiduHBGONqMXxKzpKcMSZyMZzjLMkZYyIj2Nu6jDFuFuP3ySU4HYAxJv5JmFPI7YgMEpFPRCRfRO6tZPndIrJFRD4SkZUi0i7UNi3JGWMiVwNZTkQSgZnAYCALuE5EsoKKvQ/kqGo34GXgsVChWZIzxkRIwv5fCH2AfFX9XFWPAvOAEYEFVPWfqnrYP7sWSAu1UUtyxpiIlD6FJJwphFRgd8B8gf+7qtwCvBFqo3bhwRgTufAvPDQXkfUB87NUdVY1W9FKdycyFsgBBoTaoSU5Y0zETmDEw15VzaliWQHQJmA+DSiqsC+RS4HJwABVLQ61Q+uuGmMiJhLeFMI6oJOIpItIfWAMsLD8fqQH8L/AcFXdE05sluSMMRGriVtIVLUEuB1YBmwFXlLVzSIyVUSG+4tNB04DFojIByKysIrNlbHuqjEmMuHeBBcGVV0CLAn6bkrA50tPdJuW5IwxEfFdXY3dIQ+W5IwxEYvdFGdJzhhTE2I4y1mSM8ZELJYfmum6q6vLly2lW3ZnsjMzmP7YIxWWFxcXM/ZHo8nOzOD8fn3ZtXNn2bLpj04jOzODbtmdWbF8WS1GHR631u2ZB65n18pprF9wf5Vl/vDLkXyc+wB58++je+bxkTzXD+vLptwpbMqdwvXD+tZGuCfErccsWA3dQhIVrkpyXq+XSXfcRu6iN3j/oy0smPciW7dsKVfmuWdnk5yUzOZt+Uy88y4m3/8rALZu2cKC+fPY+OFmFi5eyp0Tb8Xr9TpRjUq5uW5zF61lxG0zq1x++XlZdGzbgq4jfsvtD7/IU/ePASC5ySlMnjCYC254nPPHTmfyhMEkNW5UW2GH5OZjFqymnkISDa5Kcuvy8ujYMYP0Dh2oX78+o0aPYfGi3HJlFi/K5fobbgLg6mtGsuofK1FVFi/KZdToMTRo0ID26el07JjBurw8J6pRKTfXbc3G7Xxz8HCVy4cO6MbfF/vizdu0k6aNG9GqeRMu69eFlWu3sf/QYQ58+z0r125jYP/gh1Y4x83HLFDpQzPDmZzgqiRXVFRIWtrxUSGpqWkUFhZWLNPGV8bj8dCkaVP27dtHYWHFdYuKyq/rJDfXLZSUlkkUfLm/bL7wqwOktEwipUUSBV8FfL/nACktkpwIsVJ15piF2VV1qrvqqgsPqhXH8gb/9aiyTBjrOsnNdQulslBVtfLvKx/P7Yi6dMxiNzKXteRSU9MoKDj+pJbCwgJSUlIqltntK1NSUsKhgwdp1qwZqWkV123duvy6TnJz3UIp/OoAaa2Sy+ZTz0g+6yA7AAAI1UlEQVTii68PUrjnAGlnBHzf0vd9rKhTxyyGT8q5Ksnl9O5Nfv5n7Nyxg6NHj7Jg/jyGDB1ersyQocN5Ye4cAF595WUGXHQxIsKQocNZMH8excXF7Nyxg/z8z+jdp48T1aiUm+sWyutvbeJHQ33x9jmrPYe++54v9x5ixb+2cum5mSQ1bkRS40Zcem4mK/611eFoj6s7x6zGHpoZFa7qrno8HmY8+TTDhlyO1+vlpnHjycrOZuqDU+jZK4ehw4YzbvwtjB93A9mZGSQnN2PuC/MAyMrO5ppR19KjWxYej4cnnppJYmKiwzU6zs11mzNtHOf36kTzpNPIX/oQDz2zhHoeX3x/eXk1S1dv5vLzstm88AEOH/mBnzz4NwD2HzrMtD8vZfXffgnA72ctZf+hqi9g1DY3H7NApQ/NjFVS2TkBp/TqlaNr3lsfuqCJGcm9b3c6hKjYv+5pp0OIiv59c9iwYX2NpqRu3XvpwpVrwiqb3rzRhmqeJxcVrmrJGWOcEcsjHizJGWMiFsMXfi3JGWMiF8M5zpKcMSZCDt7oGw5LcsaYiJQO64pVluSMMRGL3RRnSc4YUwNiuCFnSc4YEzm7hcQY426xm+MsyRljIhfDOc6SnDEmMiL2SkJjjNvFbo6zJGeMiVwM5zhLcsaYyMVwb9WSnDEmUs49EDMcluSMMRHxDetyOoqqWZIzxkTMkpwxxtWsu2qMcS971JIxxs0cfNtgWCzJGWMiF8NZzpKcMSZisTysy1UvlzbGOEPCnEJuR2SQiHwiIvkicm8lyxuIyHz/8vdEpH2obVqSM8ZErgaynIgkAjOBwUAWcJ2IZAUVuwXYr6oZwAzg0VChWZIzxkRMwvxfCH2AfFX9XFWPAvOAEUFlRgBz/J9fBi6REC+YiKlzchs3btjbqJ7sqqXdNQf21tK+apNb6wW1WLdG9WbWxm6c0K6mN/j+xg3LTqkvzcMs3lBE1gfMz1LVWf7PqcDugGUFQN+g9cvKqGqJiBwETqea30VMJTlVbVFb+xKR9aqaU1v7qy1urRe4u27xTFUH1dCmKmuR6UmUKce6q8aYWFEAtAmYTwOKqiojIh6gKfBNdRu1JGeMiRXrgE4iki4i9YExwMKgMguBm/yfRwL/UNVqW3Ix1V2tZbNCF4lLbq0XuLtudZ7/HNvtwDIgEXhWVTeLyFRgvaouBGYDc0UkH18Lbkyo7UqIJGiMMXHNuqvGGFezJGeMcTVLcsYYV6tzSc4/dMRVRCRDRHJEpIHTsdQkEckWkQEicrrTsZj4VWeSnIicCaCqXjclOhEZCrwKTAeeK61nvBORwcCLwF3A8yLSyuGQTJyqE0nOnwg+EJG/g3sSnYj0Ax4HblLVi4D9QIUnN8QbEbkQeBL4L1W9EjgKdHU0KBO3XH8LiYicCryCr7XTD/Co6lj/skRV9ToZXyT8Se5MVX3OP98C+DMwWlWLnYwtEiLSBWilqv/0t+A2AnnAV8AK4JVQN4AaU8r1SQ5ARFKAQ0BD4BngSGmii2f+1uipqnrI/7k1sAgYqKpfi8jpqrrP2SgjIyKT8f1OHxaRm4FBwO2q+rXDoZk4USeSXCD/SexZwPeqOlZEegKHVXWbw6FFxD+OryGQq6qXiMj1wHnA3ar6vbPR1RwRWQL8WlU3Oh2LiQ914pxcIH/L5ifADyKyDZgPfOdsVJFT1RJV/Q7YLSLTgLuBP8Vzggt+TpiIXAOcQcVB28ZUqU6OXVXVvSLyEb4nkF6mqgVOxxQpf0KoB5zv/+8lqvqZs1FFpvS8m//WmLH4EvdoVf3S0cBMXKmTSU5EkoEr8J272uR0PDXBnxCOishDwLp4T3BBjgFfAFer6idOB2PiS507J1dKRBqq6hGn46hpIiJ25dGY4+pskjPG1A117sKDMaZusSRnjHE1S3LGGFezJGeMcTVLcnFERLwi8oGIfCwiC0TklAi2daGILPZ/Hi4iVQ7sF5EkEbn1JPbxoIj8Itzvg8o8JyIjT2Bf7UXk4xON0bifJbn48r2qdlfVrviezPHTwIXic8LHVFUXquoj1RRJAk44yRkTCyzJxa93gAx/C2ariPwJ39M62ojIQBF5V0Q2+lt8pwGIyCAR2SYiq4GrSzckIuNE5Gn/5zNE5DUR+dA/9QMeATr6W5HT/eXuEZF1IvKRiPw2YFuTReQTEXkT6ByqEiLyY/92PhSRV4Jap5eKyDsi8qn/cVmISKKITA/Y908i/Yc07mZJLg75B+MPBkpHa3QGnlfVHsB/gF8Dl6pqT2A9cLeINMT3GKZh+IZ+VfUQyqeAt1T1bKAnsBnfM+q2+1uR94jIQKAT0AfoDvQSkQtEpBe+V8T1wJdEe4dRnVdVtbd/f1uBWwKWtQcGAEOAZ/x1uAU4qKq9/dv/sYikh7EfU0fVyWFdcayRiHzg//wOvndQpgC7VHWt//tzgCxgjX98e33gXSAT2FE63EtE/gZMqGQfFwM3gu/hosBB/zC4QAP90/v++dPwJb3GwGuqeti/j+AXA1emq4g8jK9LfBq+d26WeklVjwGficjn/joMBLoFnK9r6t/3p2Hsy9RBluTiy/eq2j3wC38i+0/gV8AKVb0uqFx3oKaGtwgwTVX/N2gfk05iH88BV6rqhyIyDrgwYFnwttS/74mqGpgMEZH2J7hfU0dYd9V91gL9RSQDQERO8b/3YRuQLiId/eWuq2L9lcDP/OsmikgT4Ft8rbRSy4DxAef6UkWkJfA2cJWINBKRxvi6xqE0Br4QkXrA9UHLRolIgj/mDsAn/n3/zF8eETlTfE9/NqZS1pJzGf8TgccBL8rxt3f9WlU/FZEJwOsishdYTeXvTbgTmCUitwBe4Geq+q6IrPHfovGG/7xcF+Bdf0vyO2Csqm4UkfnAB8AufF3qUH4DvOcvv4nyyfQT4C18z5D7qaoeEZG/4DtXt9H/eKmvgSvD+9cxdZEN0DfGuJp1V40xrmZJzhjjapbkjDGuZknOGONqluSMMa5mSc4Y42qW5Iwxrvb/9SzPlQHR0xAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b4716ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def main( X_train, y_train, X_test, y_test, smoothing=1e-4):\n",
    "    # X = words, Y = POS tags\n",
    "    Xtrain, Ytrain, Xtest, Ytest, word2idx = get_data(X_train, y_train, X_test,y_test, split_sequences=True)\n",
    "    V = len(word2idx) + 1\n",
    "\n",
    "    # find hidden state transition matrix and pi\n",
    "    M = max(max(y) for y in Ytrain) + 1 #len(set(flatten(Ytrain)))\n",
    "    A = np.ones((M, M))*smoothing # add-one smoothing\n",
    "    pi = np.zeros(M)\n",
    "    for y in Ytrain:\n",
    "        pi[y[0]] += 1\n",
    "        for i in range(len(y)-1):\n",
    "            A[y[i], y[i+1]] += 1\n",
    "    # turn it into a probability matrix\n",
    "    A /= A.sum(axis=1, keepdims=True)\n",
    "    pi /= pi.sum()\n",
    "\n",
    "    # find the observation matrix\n",
    "    B = np.ones((M, V))*smoothing # add-one smoothing\n",
    "    for x, y in zip(Xtrain, Ytrain):\n",
    "        for xi, yi in zip(x, y):\n",
    "            B[yi, xi] += 1\n",
    "    B /= B.sum(axis=1, keepdims=True)\n",
    "\n",
    "    hmm = HMM(M)\n",
    "    hmm.pi = pi\n",
    "    hmm.A = A\n",
    "    hmm.B = B\n",
    "\n",
    "    # get predictions\n",
    "    Ptrain = []\n",
    "    for x in Xtrain:\n",
    "        p = hmm.get_state_sequence(x)\n",
    "        Ptrain.append(p)\n",
    "\n",
    "    Ptest = []\n",
    "    #print(hmm.get_state_sequence(Xtrain[0]))\n",
    "    #print(Xtrain[0])\n",
    "    p = hmm.get_state_sequence(Xtrain[0])\n",
    "    #print(p)\n",
    "    for x in Xtest:\n",
    "        if x:\n",
    "            p = hmm.get_state_sequence(x)\n",
    "            Ptest.append(p)\n",
    "    labels=set(flatten(Ytest))\n",
    "    #print(labels)\n",
    "    \n",
    "    # print results\n",
    "    #print(\"train accuracy:\", accuracy(Ytrain, Ptrain))\n",
    "    print(\"The test accuracy and F1-Score:\")\n",
    "    print(\"*********************************************************************\")\n",
    "    print(\"test accuracy:\", accuracy(Ytest, Ptest))\n",
    "    print(\"*********************************************************************\")\n",
    "    #print(\"train f1:\", total_f1_score(Ytrain, Ptrain, labels))\n",
    "    print(\"test f1:\", total_f1_score(Ytest, Ptest, labels))\n",
    "    \n",
    "    cnf_matrix = total_conf_matrix(Ytest, Ptest)\n",
    "    plt.figure()\n",
    "\n",
    "    plot_confusion_matrix(cnf_matrix,classes=list(labels), normalize=True,\n",
    "                          title='ncm')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "main( X_train_new, y_train, X_test_new, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python_2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
