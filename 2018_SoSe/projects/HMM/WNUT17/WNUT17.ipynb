{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "# from hmm_class.hmmd_scaled import HMM\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=True, strip_handles=False, reduce_len=False)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for getting the word and NER tag pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples(dspath):\n",
    "    sentences = []\n",
    "    s = ''\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    poss = []\n",
    "    tot_sentences = 0\n",
    "    ners_by_position = []\n",
    "    index = 0\n",
    "    with open(dspath) as f:\n",
    "        for line in f:\n",
    "            if line.strip() != '':\n",
    "                token = line.split('\\t')[0].decode('utf-8')\n",
    "                ner = line.split('\\t')[1].replace('\\r', '').replace('\\n', '').decode('utf-8')\n",
    "                '''\n",
    "                if ner in definitions.NER_TAGS_ORG:\n",
    "                    ner = 'ORG'\n",
    "                elif ner in definitions.NER_TAGS_LOC:\n",
    "                    ner = 'LOC'\n",
    "                elif ner in definitions.NER_TAGS_PER:\n",
    "                    ner = 'PER'\n",
    "                else :\n",
    "                    ner = 'O'\n",
    "                '''\n",
    "                #ners_by_position.append([index, len(token), ner])\n",
    "                index += len(token) + 1\n",
    "            if line.strip() == '':\n",
    "                if len(tokens) != 0:\n",
    "                    #poss = [x[1].decode('utf-8') for x in nltk.pos_tag(nltk.word_tokenize(s[:-1]))]\n",
    "                    poss = [x[1].decode('utf-8') for x in nltk.pos_tag(tknzr.tokenize(s[:-1]))]\n",
    "\n",
    "\n",
    "                    #if len(poss) == len(tokens): # tokenization doesnt affect position of NERs, i.e., same tokenization\n",
    "                    sentences.append(zip(tokens, poss, ners))\n",
    "                    #else:\n",
    "                    #    aux = 0\n",
    "                    #    for i in range(len()):\n",
    "                    #        if aux <= tokens[i]\n",
    "\n",
    "                    tokens = []\n",
    "                    ners = []\n",
    "                    s = ''\n",
    "                    tot_sentences += 1\n",
    "            else:\n",
    "                s += token + ' '\n",
    "                tokens.append(token)\n",
    "                ners.append(ner)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to creating the feature vectors from the wrods and applying the classification on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1].encode(\"utf-8\")\n",
    "\n",
    "    #my addition\n",
    "    ner_class = sent[i][2].encode(\"utf-8\")\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2].encode(\"utf-8\"),\n",
    "        'stop_word': word in stop,\n",
    "        'hyphen': '-' in word,\n",
    "        'size_small': True if len(word) <= 2 else False,\n",
    "        'stemmer_lanc': lancaster_stemmer.stem(word),\n",
    "\n",
    "        #my addition\n",
    "        #'NER_class': ner_class\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def word2features_new(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1].encode(\"utf-8\")\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2].encode(\"utf-8\"),\n",
    "        'stop_word': word in stop,\n",
    "        'hyphen': '-' in word,\n",
    "        'size_small': True if len(word) <= 2 else False,\n",
    "        'stemmer_lanc': lancaster_stemmer.stem(word),\n",
    "        'klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "        'klass': tf_idf_clone.predict([word])[0],\n",
    "        'klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "        'klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '-1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '-1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '-1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '-1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '+1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '+1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '+1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '+1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def remove_extra_features(X_train_new, X_test_new):\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    for sentence in X_train_new:\n",
    "        tmp = []\n",
    "        for token in sentence:\n",
    "            del token['klass']\n",
    "            del token['klass_1']\n",
    "            del token['klass_2']\n",
    "            del token['klass_3']\n",
    "\n",
    "            if '+1:klass' in token:\n",
    "                del token['+1:klass']\n",
    "                del token['+1:klass_1']\n",
    "                del token['+1:klass_2']\n",
    "                del token['+1:klass_3']\n",
    "\n",
    "            if '-1:klass' in token:\n",
    "                del token['-1:klass']\n",
    "                del token['-1:klass_1']\n",
    "                del token['-1:klass_2']\n",
    "                del token['-1:klass_3']\n",
    "            tmp.append(token)\n",
    "            X_train.append(tmp)\n",
    "\n",
    "    for sentence in X_test_new:\n",
    "            tmp = []\n",
    "            for token in sentence:\n",
    "\n",
    "                del token['klass']\n",
    "                del token['klass_1']\n",
    "                del token['klass_2']\n",
    "                del token['klass_3']\n",
    "\n",
    "                if '+1:klass' in token:\n",
    "                    del token['+1:klass']\n",
    "                    del token['+1:klass_1']\n",
    "                    del token['+1:klass_2']\n",
    "                    del token['+1:klass_3']\n",
    "\n",
    "                if '-1:klass' in token:\n",
    "                    del token['-1:klass']\n",
    "                    del token['-1:klass_1']\n",
    "                    del token['-1:klass_2']\n",
    "                    del token['-1:klass_3']\n",
    "                tmp.append(token)\n",
    "                X_test.append(tmp)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def group_labels(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok.find(\"geo-loc\") != -1 or tok.find(\"location\") != -1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok.find(\"company\") != -1 or tok.find(\"corporation\") != -1:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok.find(\"person\") != -1:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def group_labels_num(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok == 1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok == 2:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok == 3:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features_new(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label.encode(\"utf-8\") for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "#print(\"i am here1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the before created dataset (saves time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_clone_1 = joblib.load('tf-idf+svm_1.pkl')\n",
    "tf_idf_clone_2 = joblib.load('tf-idf+svm_2.pkl')\n",
    "tf_idf_clone_3 = joblib.load('tf-idf+svm_3.pkl')\n",
    "tf_idf_clone = joblib.load('tf-idf+svm_new.pkl')\n",
    "\n",
    "\n",
    "X_train = joblib.load('X_train.pkl')\n",
    "X_train_new = joblib.load('X_train_new.pkl')\n",
    "y_train = joblib.load('y_train.pkl')\n",
    "\n",
    "X_test = joblib.load('X_test.pkl')\n",
    "X_test_new = joblib.load('X_test_new.pkl')\n",
    "y_test = joblib.load('y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HMM part - First getting the frequencies of the different entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, labels, test,testy, split_sequences=False):\n",
    "    # if not os.path.exists('chunking'):\n",
    "    #     print(\"Please create a folder in your local directory called 'chunking'\")\n",
    "    #     print(\"train.txt and test.txt should be stored in there.\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/train.txt'):\n",
    "    #     print(\"train.txt is not in chunking/train.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/test.txt'):\n",
    "    #     print(\"test.txt is not in chunking/test.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "\n",
    "    word2idx = {}\n",
    "    tag2idx = {}\n",
    "    word_idx = 0\n",
    "    tag_idx = 0\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    \n",
    "\n",
    "    for line1,line2 in itertools.izip(train,labels):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #print(\"class\")\n",
    "                #r = line.split()\n",
    "\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2\n",
    "            #print(tag)\n",
    "            #word, tag = r\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            currentX.append(word2idx[word])\n",
    "\n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtrain.append(currentX)\n",
    "            Ytrain.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "\n",
    "    # load and score test data\n",
    "    Xtest = []\n",
    "    Ytest = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line1,line2 in itertools.izip(test,testy):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #r = line.split()\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2#[:2]\n",
    "            #word, tag, = r\n",
    "            if word in word2idx:\n",
    "                currentX.append(word2idx[word])\n",
    "            else:\n",
    "                currentX.append(word_idx)  # use this as unknown\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtest.append(currentX)\n",
    "            Ytest.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx\n",
    "\n",
    "def random_normalized(d1, d2):\n",
    "    x = np.random.random((d1, d2))\n",
    "    return x / x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM model creation - train the HMM, find the hidden state matrix, create observation matrix, Baum-Welch and Viterbi used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, M):\n",
    "        self.M = M  # number of hidden states\n",
    "\n",
    "    def fit(self, X, max_iter=30):\n",
    "        np.random.seed(123)\n",
    "        # train the HMM model using the Baum-Welch algorithm\n",
    "        # a specific instance of the expectation-maximization algorithm\n",
    "\n",
    "        # determine V, the vocabulary size\n",
    "        # assume observables are already integers from 0..V-1\n",
    "        # X is a jagged array of observed sequences\n",
    "        V = max(max(x) for x in X) + 1\n",
    "        N = len(X)\n",
    "\n",
    "        self.pi = np.ones(self.M) / self.M  # initial state distribution\n",
    "        self.A = random_normalized(self.M, self.M)  # state transition matrix\n",
    "        self.B = random_normalized(self.M, V)  # output distribution\n",
    "\n",
    "        print(\"initial A:\", self.A)\n",
    "        print(\"initial B:\", self.B)\n",
    "\n",
    "        costs = []\n",
    "        for it in range(max_iter):\n",
    "            if it % 10 == 0:\n",
    "                print(\"it:\", it)\n",
    "            # alpha1 = np.zeros((N, self.M))\n",
    "            alphas = []\n",
    "            betas = []\n",
    "            scales = []\n",
    "            logP = np.zeros(N)\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                scale = np.zeros(T)\n",
    "                # alpha1[n] = self.pi*self.B[:,x[0]]\n",
    "                alpha = np.zeros((T, self.M))\n",
    "                alpha[0] = self.pi * self.B[:, x[0]]\n",
    "                scale[0] = alpha[0].sum()\n",
    "                alpha[0] /= scale[0]\n",
    "                for t in range(1, T):\n",
    "                    alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "                    scale[t] = alpha_t_prime.sum()\n",
    "                    alpha[t] = alpha_t_prime / scale[t]\n",
    "                logP[n] = np.log(scale).sum()\n",
    "                alphas.append(alpha)\n",
    "                scales.append(scale)\n",
    "\n",
    "                beta = np.zeros((T, self.M))\n",
    "                beta[-1] = 1\n",
    "                for t in range(T - 2, -1, -1):\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t + 1]] * beta[t + 1]) / scale[t + 1]\n",
    "                betas.append(beta)\n",
    "\n",
    "            cost = np.sum(logP)\n",
    "            costs.append(cost)\n",
    "\n",
    "            # now re-estimate pi, A, B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0]) for n in range(N)) / N\n",
    "\n",
    "            den1 = np.zeros((self.M, 1))\n",
    "            den2 = np.zeros((self.M, 1))\n",
    "            a_num = np.zeros((self.M, self.M))\n",
    "            b_num = np.zeros((self.M, V))\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                den1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T\n",
    "                den2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T\n",
    "\n",
    "                # numerator for A\n",
    "                # a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T - 1):\n",
    "                            a_num[i, j] += alphas[n][t, i] * betas[n][t + 1, j] * self.A[i, j] * self.B[j, x[t + 1]] / \\\n",
    "                                           scales[n][t + 1]\n",
    "                # a_num += a_num_n\n",
    "\n",
    "                # numerator for B\n",
    "                # for i in range(self.M):\n",
    "                #     for j in range(V):\n",
    "                #         for t in range(T):\n",
    "                #             if x[t] == j:\n",
    "                #                 b_num[i,j] += alphas[n][t][i] * betas[n][t][i]\n",
    "                for i in range(self.M):\n",
    "                    for t in range(T):\n",
    "                        b_num[i, x[t]] += alphas[n][t, i] * betas[n][t, i]\n",
    "            self.A = a_num / den1\n",
    "            self.B = b_num / den2\n",
    "        print(\"A:\", self.A)\n",
    "        print(\"B:\", self.B)\n",
    "        print(\"pi:\", self.pi)\n",
    "\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        # returns log P(x | model)\n",
    "        # using the forward part of the forward-backward algorithm\n",
    "        T = len(x)\n",
    "        scale = np.zeros(T)\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi * self.B[:, x[0]]\n",
    "        scale[0] = alpha[0].sum()\n",
    "        alpha[0] /= scale[0]\n",
    "        for t in range(1, T):\n",
    "            alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "            scale[t] = alpha_t_prime.sum()\n",
    "            alpha[t] = alpha_t_prime / scale[t]\n",
    "        return np.log(scale).sum()\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        return np.array([self.log_likelihood(x) for x in X])\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        # returns the most likely state sequence given observed sequence x\n",
    "        # using the Viterbi algorithm\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        #print(x)\n",
    "        delta[0] = np.log(self.pi) + np.log(self.B[:, x[0]])\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.M):\n",
    "                delta[t, j] = np.max(delta[t - 1] + np.log(self.A[:, j])) + np.log(self.B[j, x[t]])\n",
    "                psi[t, j] = np.argmax(delta[t - 1] + np.log(self.A[:, j]))\n",
    "\n",
    "        # backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T - 1] = np.argmax(delta[T - 1])\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            states[t] = psi[t + 1, states[t + 1]]\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics - Accuracy, F1-Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for t, y in zip(T, Y):\n",
    "        #print(\"miamano\")\n",
    "        #print(t,y)\n",
    "        for x, z in zip(t, y):\n",
    "            if x != 0:\n",
    "                if x == z:\n",
    "                    n_correct += 1\n",
    "                n_total += 1\n",
    "        #n_correct += np.sum(t == y)\n",
    "        #print(\"The sum\", np.sum(t == y))\n",
    "        #n_total += len(y)\n",
    "    return float(n_correct) / n_total\n",
    "\n",
    "\n",
    "def total_f1_score(T, Y, labels):\n",
    "    # inputs are lists of lists\n",
    "    \n",
    "\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    #print(\"Try to remove: \", labels)\n",
    "    labels.discard(0)\n",
    "    return f1_score(T, Y, labels=list(labels), average=None).mean()\n",
    "\n",
    "def total_conf_matrix(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    return confusion_matrix(T, Y)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HMM class and getting the prediction and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test accuracy:', 0.22945736434108527)\n",
      "('test f1:', 0.007104803332128598)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.03701262 0.91296321 0.05002418 0.        ]\n",
      " [0.         0.95333333 0.04666667 0.        ]\n",
      " [0.         0.92424242 0.07575758 0.        ]\n",
      " [0.         0.93006993 0.06993007 0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49d5f15dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def main( X_train, y_train, X_test, y_test, smoothing=1e-4):\n",
    "    # X = words, Y = POS tags\n",
    "    Xtrain, Ytrain, Xtest, Ytest, word2idx = get_data(X_train, y_train, X_test,y_test, split_sequences=True)\n",
    "    V = len(word2idx) + 1\n",
    "\n",
    "    # find hidden state transition matrix and pi\n",
    "    M = max(max(y) for y in Ytrain) + 1 #len(set(flatten(Ytrain)))\n",
    "    A = np.ones((M, M))*smoothing # add-one smoothing\n",
    "    pi = np.zeros(M)\n",
    "    for y in Ytrain:\n",
    "        pi[y[0]] += 1\n",
    "        for i in range(len(y)-1):\n",
    "            A[y[i], y[i+1]] += 1\n",
    "    # turn it into a probability matrix\n",
    "    A /= A.sum(axis=1, keepdims=True)\n",
    "    pi /= pi.sum()\n",
    "\n",
    "    # find the observation matrix\n",
    "    B = np.ones((M, V))*smoothing # add-one smoothing\n",
    "    for x, y in zip(Xtrain, Ytrain):\n",
    "        for xi, yi in zip(x, y):\n",
    "            B[yi, xi] += 1\n",
    "    B /= B.sum(axis=1, keepdims=True)\n",
    "\n",
    "    hmm = HMM(M)\n",
    "    hmm.pi = pi\n",
    "    hmm.A = A\n",
    "    hmm.B = B\n",
    "\n",
    "    # get predictions\n",
    "    Ptrain = []\n",
    "    for x in Xtrain:\n",
    "        p = hmm.get_state_sequence(x)\n",
    "        Ptrain.append(p)\n",
    "\n",
    "    Ptest = []\n",
    "    #print(hmm.get_state_sequence(Xtrain[0]))\n",
    "    #print(Xtrain[0])\n",
    "    p = hmm.get_state_sequence(Xtrain[0])\n",
    "    #print(p)\n",
    "    for x in Xtest:\n",
    "        if x:\n",
    "            p = hmm.get_state_sequence(x)\n",
    "            Ptest.append(p)\n",
    "    labels=set(flatten(Ytest))\n",
    "    #print(labels)\n",
    "    \n",
    "    # print results\n",
    "    #print(\"train accuracy:\", accuracy(Ytrain, Ptrain))\n",
    "    print(\"The test accuracy and F1-Score:\")\n",
    "    print(\"*********************************************************************\")\n",
    "    print(\"test accuracy:\", accuracy(Ytest, Ptest))\n",
    "    print(\"*********************************************************************\")\n",
    "    #print(\"train f1:\", total_f1_score(Ytrain, Ptrain, labels))\n",
    "    print(\"test f1:\", total_f1_score(Ytest, Ptest, labels))\n",
    "    \n",
    "    cnf_matrix = total_conf_matrix(Ytest, Ptest)\n",
    "    plt.figure()\n",
    "\n",
    "    plot_confusion_matrix(cnf_matrix,classes=list(labels), normalize=True,\n",
    "                          title='ncm')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "main( X_train_new, y_train, X_test_new, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python_2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
