{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "# from hmm_class.hmmd_scaled import HMM\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=True, strip_handles=False, reduce_len=False)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# from pos_baseline import get_data\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "import nltk\n",
    "from numpy.ma import average\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import  f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict,KFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.base import clone\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from CMUTweetTagger import runtagger_parse\n",
    "#from spacy.language import Tokenizer,GoldParse\n",
    "#from spacy.tokenizer import Tokenizer\n",
    "#from spacy.attrs import ORTH, LEMMA\n",
    "#import spacy\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "#tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=True, strip_handles=False, reduce_len=False)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for getting the word and NER tag pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples(dspath):\n",
    "    sentences = []\n",
    "    s = ''\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    poss = []\n",
    "    tot_sentences = 0\n",
    "    ners_by_position = []\n",
    "    index = 0\n",
    "    with open(dspath) as f:\n",
    "        for line in f:\n",
    "            if line.strip() != '':\n",
    "                token = line.split('\\t')[0].decode('utf-8')\n",
    "                ner = line.split('\\t')[1].replace('\\r', '').replace('\\n', '').decode('utf-8')\n",
    "                '''\n",
    "                if ner in definitions.NER_TAGS_ORG:\n",
    "                    ner = 'ORG'\n",
    "                elif ner in definitions.NER_TAGS_LOC:\n",
    "                    ner = 'LOC'\n",
    "                elif ner in definitions.NER_TAGS_PER:\n",
    "                    ner = 'PER'\n",
    "                else :\n",
    "                    ner = 'O'\n",
    "                '''\n",
    "                #ners_by_position.append([index, len(token), ner])\n",
    "                index += len(token) + 1\n",
    "            if line.strip() == '':\n",
    "                if len(tokens) != 0:\n",
    "                    #poss = [x[1].decode('utf-8') for x in nltk.pos_tag(nltk.word_tokenize(s[:-1]))]\n",
    "\n",
    "                    tknz = tokenizer(s)\n",
    "                    tknz2 = []\n",
    "                    # print len(tknz)\n",
    "                    for x in tknz:\n",
    "                        x = str(x).decode('utf-8')\n",
    "                        if x == u\"  \":\n",
    "                            tknz2.append(u\" \")\n",
    "                            tknz2.append(u\" \")\n",
    "                        else:\n",
    "                            tknz2.append(x)\n",
    "\n",
    "                    # print tknz2\n",
    "                    poss = [x[1].decode('utf-8') for x in nltk.pos_tag(tknz2)]\n",
    "\n",
    "                    # gold = GoldParse(doc, words=words, tags=tags)\n",
    "\n",
    "                    # assert len(poss) == len(tokens) == len(ners)\n",
    "\n",
    "                    sentences.append(zip(tokens, poss, ners))\n",
    "                    #else:\n",
    "                    #    aux = 0\n",
    "                    #    for i in range(len()):\n",
    "                    #        if aux <= tokens[i]\n",
    "                    # if len(poss) != len(tokens) or len(poss) != len(ners):\n",
    "                    #     print (poss)\n",
    "                    #     print tknz2, len(tknz2)\n",
    "                    #     print(tokens), len(tokens)\n",
    "                    #     print(ners)\n",
    "                    #     print \"---------------------------\"\n",
    "                    tokens = []\n",
    "                    ners = []\n",
    "                    s = ''\n",
    "                    tot_sentences += 1\n",
    "\n",
    "\n",
    "            else:\n",
    "                s += token + ' '\n",
    "                tokens.append(token)\n",
    "                ners.append(ner)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to creating the feature vectors from the wrods and applying the classification on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features_new(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1].encode(\"utf-8\")\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2].encode(\"utf-8\"),\n",
    "        'stop_word': word in stop,\n",
    "        'hyphen': '-' in word,\n",
    "        'size_small': True if len(word) <= 2 else False,\n",
    "        'stemmer_lanc': lancaster_stemmer.stem(word),\n",
    "        'klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "        'klass': tf_idf_clone.predict([word])[0],\n",
    "        'klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "        'klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '-1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '-1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '-1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '-1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '+1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '+1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '+1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '+1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def group_labels(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok.find(\"geo-loc\") != -1 or tok.find(\"location\") != -1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok.find(\"company\") != -1 or tok.find(\"corporation\") != -1:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok.find(\"person\") != -1:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def group_labels_num(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok == 1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok == 2:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok == 3:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2features_new(sent):\n",
    "    return [word2features_new(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label.encode(\"utf-8\") for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "\n",
    "\n",
    "#X_train = [sent2features(s) for s in train_sents]\n",
    "\n",
    "#X_test = [sent2features(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the before created dataset (saves time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = joblib.load('X_train.pkl')\n",
    "X_train_new = joblib.load('X_train_new.pkl')\n",
    "y_train_raw = joblib.load('y_train.pkl')\n",
    "\n",
    "y_train = group_labels(y_train_raw)\n",
    "\n",
    "tf_idf_clone_1 = joblib.load('tf-idf+svm_1.pkl')\n",
    "tf_idf_clone_2 = joblib.load('tf-idf+svm_2.pkl')\n",
    "tf_idf_clone_3 = joblib.load('tf-idf+svm_3.pkl')\n",
    "tf_idf_clone = joblib.load('tf-idf+svm_new.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = int(len(X_train_new)*0.8)\n",
    "X_train_splitted = X_train_new[:split_ratio]\n",
    "y_train_splitted = y_train[:split_ratio]\n",
    "\n",
    "X_test = X_train_new[split_ratio:]\n",
    "y_test = y_train[split_ratio:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HMM part - First getting the frequencies of the different entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, labels, test,testy, split_sequences=False):\n",
    "    # if not os.path.exists('chunking'):\n",
    "    #     print(\"Please create a folder in your local directory called 'chunking'\")\n",
    "    #     print(\"train.txt and test.txt should be stored in there.\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/train.txt'):\n",
    "    #     print(\"train.txt is not in chunking/train.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/test.txt'):\n",
    "    #     print(\"test.txt is not in chunking/test.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "\n",
    "    word2idx = {}\n",
    "    tag2idx = {}\n",
    "    word_idx = 0\n",
    "    tag_idx = 0\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    \n",
    "\n",
    "    for line1,line2 in itertools.izip(train,labels):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #print(\"class\")\n",
    "                #r = line.split()\n",
    "\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2\n",
    "            #print(tag)\n",
    "            #word, tag = r\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            currentX.append(word2idx[word])\n",
    "\n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtrain.append(currentX)\n",
    "            Ytrain.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "\n",
    "    # load and score test data\n",
    "    Xtest = []\n",
    "    Ytest = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line1,line2 in itertools.izip(test,testy):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #r = line.split()\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2#[:2]\n",
    "            #word, tag, = r\n",
    "            if word in word2idx:\n",
    "                currentX.append(word2idx[word])\n",
    "            else:\n",
    "                currentX.append(word_idx)  # use this as unknown\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtest.append(currentX)\n",
    "            Ytest.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx\n",
    "\n",
    "def random_normalized(d1, d2):\n",
    "    x = np.random.random((d1, d2))\n",
    "    return x / x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM model creation - train the HMM, find the hidden state matrix, create observation matrix, Baum-Welch and Viterbi used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, M):\n",
    "        self.M = M  # number of hidden states\n",
    "\n",
    "    def fit(self, X, max_iter=30):\n",
    "        np.random.seed(123)\n",
    "        # train the HMM model using the Baum-Welch algorithm\n",
    "        # a specific instance of the expectation-maximization algorithm\n",
    "\n",
    "        # determine V, the vocabulary size\n",
    "        # assume observables are already integers from 0..V-1\n",
    "        # X is a jagged array of observed sequences\n",
    "        V = max(max(x) for x in X) + 1\n",
    "        N = len(X)\n",
    "\n",
    "        self.pi = np.ones(self.M) / self.M  # initial state distribution\n",
    "        self.A = random_normalized(self.M, self.M)  # state transition matrix\n",
    "        self.B = random_normalized(self.M, V)  # output distribution\n",
    "\n",
    "        print(\"initial A:\", self.A)\n",
    "        print(\"initial B:\", self.B)\n",
    "\n",
    "        costs = []\n",
    "        for it in range(max_iter):\n",
    "            if it % 10 == 0:\n",
    "                print(\"it:\", it)\n",
    "            # alpha1 = np.zeros((N, self.M))\n",
    "            alphas = []\n",
    "            betas = []\n",
    "            scales = []\n",
    "            logP = np.zeros(N)\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                scale = np.zeros(T)\n",
    "                # alpha1[n] = self.pi*self.B[:,x[0]]\n",
    "                alpha = np.zeros((T, self.M))\n",
    "                alpha[0] = self.pi * self.B[:, x[0]]\n",
    "                scale[0] = alpha[0].sum()\n",
    "                alpha[0] /= scale[0]\n",
    "                for t in range(1, T):\n",
    "                    alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "                    scale[t] = alpha_t_prime.sum()\n",
    "                    alpha[t] = alpha_t_prime / scale[t]\n",
    "                logP[n] = np.log(scale).sum()\n",
    "                alphas.append(alpha)\n",
    "                scales.append(scale)\n",
    "\n",
    "                beta = np.zeros((T, self.M))\n",
    "                beta[-1] = 1\n",
    "                for t in range(T - 2, -1, -1):\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t + 1]] * beta[t + 1]) / scale[t + 1]\n",
    "                betas.append(beta)\n",
    "\n",
    "            cost = np.sum(logP)\n",
    "            costs.append(cost)\n",
    "\n",
    "            # now re-estimate pi, A, B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0]) for n in range(N)) / N\n",
    "\n",
    "            den1 = np.zeros((self.M, 1))\n",
    "            den2 = np.zeros((self.M, 1))\n",
    "            a_num = np.zeros((self.M, self.M))\n",
    "            b_num = np.zeros((self.M, V))\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                den1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T\n",
    "                den2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T\n",
    "\n",
    "                # numerator for A\n",
    "                # a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T - 1):\n",
    "                            a_num[i, j] += alphas[n][t, i] * betas[n][t + 1, j] * self.A[i, j] * self.B[j, x[t + 1]] / \\\n",
    "                                           scales[n][t + 1]\n",
    "                # a_num += a_num_n\n",
    "\n",
    "                # numerator for B\n",
    "                # for i in range(self.M):\n",
    "                #     for j in range(V):\n",
    "                #         for t in range(T):\n",
    "                #             if x[t] == j:\n",
    "                #                 b_num[i,j] += alphas[n][t][i] * betas[n][t][i]\n",
    "                for i in range(self.M):\n",
    "                    for t in range(T):\n",
    "                        b_num[i, x[t]] += alphas[n][t, i] * betas[n][t, i]\n",
    "            self.A = a_num / den1\n",
    "            self.B = b_num / den2\n",
    "        print(\"A:\", self.A)\n",
    "        print(\"B:\", self.B)\n",
    "        print(\"pi:\", self.pi)\n",
    "\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        # returns log P(x | model)\n",
    "        # using the forward part of the forward-backward algorithm\n",
    "        T = len(x)\n",
    "        scale = np.zeros(T)\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi * self.B[:, x[0]]\n",
    "        scale[0] = alpha[0].sum()\n",
    "        alpha[0] /= scale[0]\n",
    "        for t in range(1, T):\n",
    "            alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "            scale[t] = alpha_t_prime.sum()\n",
    "            alpha[t] = alpha_t_prime / scale[t]\n",
    "        return np.log(scale).sum()\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        return np.array([self.log_likelihood(x) for x in X])\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        # returns the most likely state sequence given observed sequence x\n",
    "        # using the Viterbi algorithm\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        #print(x)\n",
    "        delta[0] = np.log(self.pi) + np.log(self.B[:, x[0]])\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.M):\n",
    "                delta[t, j] = np.max(delta[t - 1] + np.log(self.A[:, j])) + np.log(self.B[j, x[t]])\n",
    "                psi[t, j] = np.argmax(delta[t - 1] + np.log(self.A[:, j]))\n",
    "\n",
    "        # backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T - 1] = np.argmax(delta[T - 1])\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            states[t] = psi[t + 1, states[t + 1]]\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics - Accuracy, F1-Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for t, y in zip(T, Y):\n",
    "        #print(\"miamano\")\n",
    "        #print(t,y)\n",
    "        for x, z in zip(t, y):\n",
    "            if x != 0:\n",
    "                if x == z:\n",
    "                    n_correct += 1\n",
    "                n_total += 1\n",
    "        #n_correct += np.sum(t == y)\n",
    "        #print(\"The sum\", np.sum(t == y))\n",
    "        #n_total += len(y)\n",
    "    return float(n_correct) / n_total\n",
    "\n",
    "\n",
    "def total_f1_score(T, Y, labels):\n",
    "    # inputs are lists of lists\n",
    "    \n",
    "\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    labels.remove(0)\n",
    "    return f1_score(T, Y, labels=list(labels), average=None).mean()\n",
    "\n",
    "def total_conf_matrix(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    return confusion_matrix(T, Y)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HMM class and getting the prediction and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test accuracy:', 0.2631578947368421)\n",
      "('test f1:', 0.12330333090073504)\n",
      "Normalized confusion matrix\n",
      "[[0.12454212 0.87201687 0.003441   0.        ]\n",
      " [0.04255319 0.95744681 0.         0.        ]\n",
      " [0.         0.87313433 0.12686567 0.        ]\n",
      " [0.         0.90588235 0.         0.09411765]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14d1b1f090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def main( X_train, y_train, X_test, y_test, smoothing=1e-4):\n",
    "    # X = words, Y = POS tags\n",
    "    Xtrain, Ytrain, Xtest, Ytest, word2idx = get_data(X_train, y_train, X_test,y_test, split_sequences=True)\n",
    "    V = len(word2idx) + 1\n",
    "\n",
    "    # find hidden state transition matrix and pi\n",
    "    M = max(max(y) for y in Ytrain) + 1 #len(set(flatten(Ytrain)))\n",
    "    A = np.ones((M, M))*smoothing # add-one smoothing\n",
    "    pi = np.zeros(M)\n",
    "    for y in Ytrain:\n",
    "        pi[y[0]] += 1\n",
    "        for i in range(len(y)-1):\n",
    "            A[y[i], y[i+1]] += 1\n",
    "    # turn it into a probability matrix\n",
    "    A /= A.sum(axis=1, keepdims=True)\n",
    "    pi /= pi.sum()\n",
    "\n",
    "    # find the observation matrix\n",
    "    B = np.ones((M, V))*smoothing # add-one smoothing\n",
    "    for x, y in zip(Xtrain, Ytrain):\n",
    "        for xi, yi in zip(x, y):\n",
    "            B[yi, xi] += 1\n",
    "    B /= B.sum(axis=1, keepdims=True)\n",
    "\n",
    "    hmm = HMM(M)\n",
    "    hmm.pi = pi\n",
    "    hmm.A = A\n",
    "    hmm.B = B\n",
    "\n",
    "    # get predictions\n",
    "    Ptrain = []\n",
    "    for x in Xtrain:\n",
    "        p = hmm.get_state_sequence(x)\n",
    "        Ptrain.append(p)\n",
    "\n",
    "    Ptest = []\n",
    "    #print(hmm.get_state_sequence(Xtrain[0]))\n",
    "    #print(Xtrain[0])\n",
    "    p = hmm.get_state_sequence(Xtrain[0])\n",
    "    #print(p)\n",
    "    for x in Xtest:\n",
    "        if x:\n",
    "            p = hmm.get_state_sequence(x)\n",
    "            Ptest.append(p)\n",
    "    \n",
    "    \n",
    "    labels=set(flatten(Ytest))\n",
    "    # print results\n",
    "    #print(\"train accuracy:\", accuracy(Ytrain, Ptrain))\n",
    "    print(\"The test accuracy and F1-Score:\")\n",
    "    print(\"*********************************************************************\")\n",
    "    print(\"test accuracy:\", accuracy(Ytest, Ptest))\n",
    "    print(\"*********************************************************************\")\n",
    "    #print(\"train f1:\", total_f1_score(Ytrain, Ptrain))\n",
    "    print(\"test f1:\", total_f1_score(Ytest, Ptest, labels))\n",
    "    \n",
    "    cnf_matrix = total_conf_matrix(Ytest, Ptest)\n",
    "    plt.figure()\n",
    "\n",
    "    plot_confusion_matrix(cnf_matrix,classes=list(labels), normalize=True,\n",
    "                          title='ncm')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "main( X_train_splitted, y_train_splitted, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python_2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
