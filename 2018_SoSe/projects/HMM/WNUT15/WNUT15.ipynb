{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "# from hmm_class.hmmd_scaled import HMM\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=True, strip_handles=False, reduce_len=False)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for getting the word and NER tag pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples(dspath):\n",
    "    sentences = []\n",
    "    s = ''\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    poss = []\n",
    "    tot_sentences = 0\n",
    "    ners_by_position = []\n",
    "    index = 0\n",
    "    with open(dspath) as f:\n",
    "        for line in f:\n",
    "            if line.strip() != '':\n",
    "                token = line.split('\\t')[0].decode('utf-8')\n",
    "                ner = line.split('\\t')[1].replace('\\r', '').replace('\\n', '').decode('utf-8')\n",
    "                '''\n",
    "                if ner in definitions.NER_TAGS_ORG:\n",
    "                    ner = 'ORG'\n",
    "                elif ner in definitions.NER_TAGS_LOC:\n",
    "                    ner = 'LOC'\n",
    "                elif ner in definitions.NER_TAGS_PER:\n",
    "                    ner = 'PER'\n",
    "                else :\n",
    "                    ner = 'O'\n",
    "                '''\n",
    "                #ners_by_position.append([index, len(token), ner])\n",
    "                index += len(token) + 1\n",
    "            if line.strip() == '':\n",
    "                if len(tokens) != 0:\n",
    "                    #poss = [x[1].decode('utf-8') for x in nltk.pos_tag(nltk.word_tokenize(s[:-1]))]\n",
    "                    poss = [x[1].decode('utf-8') for x in nltk.pos_tag(tknzr.tokenize(s[:-1]))]\n",
    "\n",
    "\n",
    "                    #if len(poss) == len(tokens): # tokenization doesnt affect position of NERs, i.e., same tokenization\n",
    "                    sentences.append(zip(tokens, poss, ners))\n",
    "                    #else:\n",
    "                    #    aux = 0\n",
    "                    #    for i in range(len()):\n",
    "                    #        if aux <= tokens[i]\n",
    "\n",
    "                    tokens = []\n",
    "                    ners = []\n",
    "                    s = ''\n",
    "                    tot_sentences += 1\n",
    "            else:\n",
    "                s += token + ' '\n",
    "                tokens.append(token)\n",
    "                ners.append(ner)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to creating the feature vectors from the wrods and applying the classification on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1].encode(\"utf-8\")\n",
    "\n",
    "    #my addition\n",
    "    ner_class = sent[i][2].encode(\"utf-8\")\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2].encode(\"utf-8\"),\n",
    "        'stop_word': word in stop,\n",
    "        'hyphen': '-' in word,\n",
    "        'size_small': True if len(word) <= 2 else False,\n",
    "        'stemmer_lanc': lancaster_stemmer.stem(word),\n",
    "\n",
    "        #my addition\n",
    "        #'NER_class': ner_class\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def word2features_new(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1].encode(\"utf-8\")\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2].encode(\"utf-8\"),\n",
    "        'stop_word': word in stop,\n",
    "        'hyphen': '-' in word,\n",
    "        'size_small': True if len(word) <= 2 else False,\n",
    "        'stemmer_lanc': lancaster_stemmer.stem(word),\n",
    "        'klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "        'klass': tf_idf_clone.predict([word])[0],\n",
    "        'klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "        'klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '-1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '-1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '-1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '-1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1].encode(\"utf-8\")\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2].encode(\"utf-8\"),\n",
    "            '+1:klass': tf_idf_clone.predict([word])[0],\n",
    "            '+1:klass_1': tf_idf_clone_1.predict([word])[0],\n",
    "            '+1:klass_2': tf_idf_clone_2.predict([word])[0],\n",
    "            '+1:klass_3': tf_idf_clone_3.predict([word])[0],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def remove_extra_features(X_train_new, X_test_new):\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    for sentence in X_train_new:\n",
    "        tmp = []\n",
    "        for token in sentence:\n",
    "            del token['klass']\n",
    "            del token['klass_1']\n",
    "            del token['klass_2']\n",
    "            del token['klass_3']\n",
    "\n",
    "            if '+1:klass' in token:\n",
    "                del token['+1:klass']\n",
    "                del token['+1:klass_1']\n",
    "                del token['+1:klass_2']\n",
    "                del token['+1:klass_3']\n",
    "\n",
    "            if '-1:klass' in token:\n",
    "                del token['-1:klass']\n",
    "                del token['-1:klass_1']\n",
    "                del token['-1:klass_2']\n",
    "                del token['-1:klass_3']\n",
    "            tmp.append(token)\n",
    "            X_train.append(tmp)\n",
    "\n",
    "    for sentence in X_test_new:\n",
    "            tmp = []\n",
    "            for token in sentence:\n",
    "\n",
    "                del token['klass']\n",
    "                del token['klass_1']\n",
    "                del token['klass_2']\n",
    "                del token['klass_3']\n",
    "\n",
    "                if '+1:klass' in token:\n",
    "                    del token['+1:klass']\n",
    "                    del token['+1:klass_1']\n",
    "                    del token['+1:klass_2']\n",
    "                    del token['+1:klass_3']\n",
    "\n",
    "                if '-1:klass' in token:\n",
    "                    del token['-1:klass']\n",
    "                    del token['-1:klass_1']\n",
    "                    del token['-1:klass_2']\n",
    "                    del token['-1:klass_3']\n",
    "                tmp.append(token)\n",
    "                X_test.append(tmp)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def group_labels(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok.find(\"geo-loc\") != -1 or tok.find(\"location\") != -1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok.find(\"company\") != -1 or tok.find(\"corporation\") != -1:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok.find(\"person\") != -1:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def group_labels_num(labels):\n",
    "    y = []\n",
    "    for string in labels:\n",
    "        temp = []\n",
    "        for tok in string:\n",
    "            if tok == 1:\n",
    "                temp.append(\"LOC\")\n",
    "            elif tok == 2:\n",
    "                temp.append(\"ORG\")\n",
    "            elif tok == 3:\n",
    "                temp.append(\"PER\")\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "\n",
    "        y.append(temp)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features_new(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label.encode(\"utf-8\") for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "#print(\"i am here1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the before created dataset (saves time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_idf_clone_1 = joblib.load('tf-idf+svm_1.pkl')\n",
    "tf_idf_clone_2 = joblib.load('tf-idf+svm_2.pkl')\n",
    "tf_idf_clone_3 = joblib.load('tf-idf+svm_3.pkl')\n",
    "tf_idf_clone = joblib.load('tf-idf+svm_new.pkl')\n",
    "\n",
    "\n",
    "X_train = joblib.load('X_train.pkl')\n",
    "X_train_new = joblib.load('X_train_new.pkl')\n",
    "y_train_raw = joblib.load('y_train.pkl')\n",
    "\n",
    "y_train = group_labels(y_train_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = int(len(X_train_new)*0.8)\n",
    "X_train_splitted = X_train_new[:split_ratio]\n",
    "y_train_splitted = y_train[:split_ratio]\n",
    "\n",
    "X_test = X_train_new[split_ratio:]\n",
    "y_test = y_train[split_ratio:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HMM part - First getting the frequencies of the different entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, labels, test,testy, split_sequences=False):\n",
    "    # if not os.path.exists('chunking'):\n",
    "    #     print(\"Please create a folder in your local directory called 'chunking'\")\n",
    "    #     print(\"train.txt and test.txt should be stored in there.\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/train.txt'):\n",
    "    #     print(\"train.txt is not in chunking/train.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "    # elif not os.path.exists('chunking/test.txt'):\n",
    "    #     print(\"test.txt is not in chunking/test.txt\")\n",
    "    #     print(\"Please check the comments to get the download link.\")\n",
    "    #     exit()\n",
    "\n",
    "    word2idx = {}\n",
    "    tag2idx = {}\n",
    "    word_idx = 0\n",
    "    tag_idx = 0\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    \n",
    "\n",
    "    for line1,line2 in itertools.izip(train,labels):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #print(\"class\")\n",
    "                #r = line.split()\n",
    "\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2\n",
    "            #print(tag)\n",
    "            #word, tag = r\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            currentX.append(word2idx[word])\n",
    "\n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtrain.append(currentX)\n",
    "            Ytrain.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "\n",
    "    # load and score test data\n",
    "    Xtest = []\n",
    "    Ytest = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line1,line2 in itertools.izip(test,testy):\n",
    "        #print(\"line\")\n",
    "        #print(line)\n",
    "        # line = line.rstrip()\n",
    "        # here should I iterate the list with the lenght and number to get the labels\n",
    "        for pair1,pair2 in itertools.izip(line1,line2):\n",
    "\n",
    "            #if pair:\n",
    "                #r = line.split()\n",
    "            word = tuple(pair1.values())\n",
    "            tag = pair2#[:2]\n",
    "            #word, tag, = r\n",
    "            if word in word2idx:\n",
    "                currentX.append(word2idx[word])\n",
    "            else:\n",
    "                currentX.append(word_idx)  # use this as unknown\n",
    "            currentY.append(tag2idx[tag])\n",
    "        if split_sequences:\n",
    "            Xtest.append(currentX)\n",
    "            Ytest.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx\n",
    "\n",
    "def random_normalized(d1, d2):\n",
    "    x = np.random.random((d1, d2))\n",
    "    return x / x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM model creation - train the HMM, find the hidden state matrix, create observation matrix, Baum-Welch and Viterbi used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, M):\n",
    "        self.M = M  # number of hidden states\n",
    "\n",
    "    def fit(self, X, max_iter=30):\n",
    "        np.random.seed(123)\n",
    "        # train the HMM model using the Baum-Welch algorithm\n",
    "        # a specific instance of the expectation-maximization algorithm\n",
    "\n",
    "        # determine V, the vocabulary size\n",
    "        # assume observables are already integers from 0..V-1\n",
    "        # X is a jagged array of observed sequences\n",
    "        V = max(max(x) for x in X) + 1\n",
    "        N = len(X)\n",
    "\n",
    "        self.pi = np.ones(self.M) / self.M  # initial state distribution\n",
    "        self.A = random_normalized(self.M, self.M)  # state transition matrix\n",
    "        self.B = random_normalized(self.M, V)  # output distribution\n",
    "\n",
    "        print(\"initial A:\", self.A)\n",
    "        print(\"initial B:\", self.B)\n",
    "\n",
    "        costs = []\n",
    "        for it in range(max_iter):\n",
    "            if it % 10 == 0:\n",
    "                print(\"it:\", it)\n",
    "            # alpha1 = np.zeros((N, self.M))\n",
    "            alphas = []\n",
    "            betas = []\n",
    "            scales = []\n",
    "            logP = np.zeros(N)\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                scale = np.zeros(T)\n",
    "                # alpha1[n] = self.pi*self.B[:,x[0]]\n",
    "                alpha = np.zeros((T, self.M))\n",
    "                alpha[0] = self.pi * self.B[:, x[0]]\n",
    "                scale[0] = alpha[0].sum()\n",
    "                alpha[0] /= scale[0]\n",
    "                for t in range(1, T):\n",
    "                    alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "                    scale[t] = alpha_t_prime.sum()\n",
    "                    alpha[t] = alpha_t_prime / scale[t]\n",
    "                logP[n] = np.log(scale).sum()\n",
    "                alphas.append(alpha)\n",
    "                scales.append(scale)\n",
    "\n",
    "                beta = np.zeros((T, self.M))\n",
    "                beta[-1] = 1\n",
    "                for t in range(T - 2, -1, -1):\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t + 1]] * beta[t + 1]) / scale[t + 1]\n",
    "                betas.append(beta)\n",
    "\n",
    "            cost = np.sum(logP)\n",
    "            costs.append(cost)\n",
    "\n",
    "            # now re-estimate pi, A, B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0]) for n in range(N)) / N\n",
    "\n",
    "            den1 = np.zeros((self.M, 1))\n",
    "            den2 = np.zeros((self.M, 1))\n",
    "            a_num = np.zeros((self.M, self.M))\n",
    "            b_num = np.zeros((self.M, V))\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                den1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T\n",
    "                den2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T\n",
    "\n",
    "                # numerator for A\n",
    "                # a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T - 1):\n",
    "                            a_num[i, j] += alphas[n][t, i] * betas[n][t + 1, j] * self.A[i, j] * self.B[j, x[t + 1]] / \\\n",
    "                                           scales[n][t + 1]\n",
    "                # a_num += a_num_n\n",
    "\n",
    "                # numerator for B\n",
    "                # for i in range(self.M):\n",
    "                #     for j in range(V):\n",
    "                #         for t in range(T):\n",
    "                #             if x[t] == j:\n",
    "                #                 b_num[i,j] += alphas[n][t][i] * betas[n][t][i]\n",
    "                for i in range(self.M):\n",
    "                    for t in range(T):\n",
    "                        b_num[i, x[t]] += alphas[n][t, i] * betas[n][t, i]\n",
    "            self.A = a_num / den1\n",
    "            self.B = b_num / den2\n",
    "        print(\"A:\", self.A)\n",
    "        print(\"B:\", self.B)\n",
    "        print(\"pi:\", self.pi)\n",
    "\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        # returns log P(x | model)\n",
    "        # using the forward part of the forward-backward algorithm\n",
    "        T = len(x)\n",
    "        scale = np.zeros(T)\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi * self.B[:, x[0]]\n",
    "        scale[0] = alpha[0].sum()\n",
    "        alpha[0] /= scale[0]\n",
    "        for t in range(1, T):\n",
    "            alpha_t_prime = alpha[t - 1].dot(self.A) * self.B[:, x[t]]\n",
    "            scale[t] = alpha_t_prime.sum()\n",
    "            alpha[t] = alpha_t_prime / scale[t]\n",
    "        return np.log(scale).sum()\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        return np.array([self.log_likelihood(x) for x in X])\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        # returns the most likely state sequence given observed sequence x\n",
    "        # using the Viterbi algorithm\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        #print(x)\n",
    "        delta[0] = np.log(self.pi) + np.log(self.B[:, x[0]])\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.M):\n",
    "                delta[t, j] = np.max(delta[t - 1] + np.log(self.A[:, j])) + np.log(self.B[j, x[t]])\n",
    "                psi[t, j] = np.argmax(delta[t - 1] + np.log(self.A[:, j]))\n",
    "\n",
    "        # backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T - 1] = np.argmax(delta[T - 1])\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            states[t] = psi[t + 1, states[t + 1]]\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics - Accuracy, F1-Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for t, y in zip(T, Y):\n",
    "        #print(\"miamano\")\n",
    "        #print(t,y)\n",
    "        for x, z in zip(t, y):\n",
    "            if x != 0:\n",
    "                if x == z:\n",
    "                    n_correct += 1\n",
    "                n_total += 1\n",
    "        #n_correct += np.sum(t == y)\n",
    "        #print(\"The sum\", np.sum(t == y))\n",
    "        #n_total += len(y)\n",
    "    return float(n_correct) / n_total\n",
    "\n",
    "\n",
    "def total_f1_score(T, Y, labels):\n",
    "    # inputs are lists of lists\n",
    "    \n",
    "\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    #print(\"Try to remove: \", labels)\n",
    "    labels.discard(0)\n",
    "    return f1_score(T, Y, labels=list(labels), average=None).mean()\n",
    "\n",
    "def total_conf_matrix(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    return confusion_matrix(T, Y)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HMM class and getting the prediction and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test accuracy:', 0.08)\n",
      "('test f1:', 0.0012940795858945324)\n",
      "Normalized confusion matrix\n",
      "[[7.29455217e-02 0.00000000e+00 9.26438904e-01 6.15574023e-04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dbe636250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def main( X_train, y_train, X_test, y_test, smoothing=1e-4):\n",
    "    # X = words, Y = POS tags\n",
    "    Xtrain, Ytrain, Xtest, Ytest, word2idx = get_data(X_train, y_train, X_test,y_test, split_sequences=True)\n",
    "    V = len(word2idx) + 1\n",
    "\n",
    "    # find hidden state transition matrix and pi\n",
    "    M = max(max(y) for y in Ytrain) + 1 #len(set(flatten(Ytrain)))\n",
    "    A = np.ones((M, M))*smoothing # add-one smoothing\n",
    "    pi = np.zeros(M)\n",
    "    for y in Ytrain:\n",
    "        pi[y[0]] += 1\n",
    "        for i in range(len(y)-1):\n",
    "            A[y[i], y[i+1]] += 1\n",
    "    # turn it into a probability matrix\n",
    "    A /= A.sum(axis=1, keepdims=True)\n",
    "    pi /= pi.sum()\n",
    "\n",
    "    # find the observation matrix\n",
    "    B = np.ones((M, V))*smoothing # add-one smoothing\n",
    "    for x, y in zip(Xtrain, Ytrain):\n",
    "        for xi, yi in zip(x, y):\n",
    "            B[yi, xi] += 1\n",
    "    B /= B.sum(axis=1, keepdims=True)\n",
    "\n",
    "    hmm = HMM(M)\n",
    "    hmm.pi = pi\n",
    "    hmm.A = A\n",
    "    hmm.B = B\n",
    "\n",
    "    # get predictions\n",
    "    Ptrain = []\n",
    "    for x in Xtrain:\n",
    "        p = hmm.get_state_sequence(x)\n",
    "        Ptrain.append(p)\n",
    "\n",
    "    Ptest = []\n",
    "    #print(hmm.get_state_sequence(Xtrain[0]))\n",
    "    #print(Xtrain[0])\n",
    "    p = hmm.get_state_sequence(Xtrain[0])\n",
    "    #print(p)\n",
    "    for x in Xtest:\n",
    "        if x:\n",
    "            p = hmm.get_state_sequence(x)\n",
    "            Ptest.append(p)\n",
    "    labels=set(flatten(Ytest))\n",
    "    #print(labels)\n",
    "    \n",
    "    # print results\n",
    "    #print(\"train accuracy:\", accuracy(Ytrain, Ptrain))\n",
    "    print(\"The test accuracy and F1-Score:\")\n",
    "    print(\"*********************************************************************\")\n",
    "    print(\"test accuracy:\", accuracy(Ytest, Ptest))\n",
    "    print(\"*********************************************************************\")\n",
    "    #print(\"train f1:\", total_f1_score(Ytrain, Ptrain, labels))\n",
    "    print(\"test f1:\", total_f1_score(Ytest, Ptest, labels))\n",
    "    \n",
    "    cnf_matrix = total_conf_matrix(Ytest, Ptest)\n",
    "    plt.figure()\n",
    "\n",
    "    plot_confusion_matrix(cnf_matrix,classes=list(labels), normalize=True,\n",
    "                          title='ncm')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "main( X_train_splitted, y_train_splitted, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python_2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
