{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for the model\n",
    "\n",
    "There is a tradeoff between number of features and memory. Furthermore, I worked on a machine with not so much memory. Therefore I played around a little bit and kept the best suited features in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import brown\n",
    "from nltk import DefaultTagger as df\n",
    "from nltk import UnigramTagger as ut\n",
    "from nltk import BigramTagger as bt\n",
    "from nltk import TrigramTagger as tg\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "\n",
    "def features(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'prefix-1': sentence[index][:1],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "      # 'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1:],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'suffix-4': sentence[index][-4:],\n",
    "      # 'suffix-5': sentence[index][-5:],\n",
    "      # 'is_all_caps': sentence[index] == sentence[index].upper(),\n",
    "      # 'contains_number': len([ch for ch in sentence[index] if ch.isdigit()])>0,\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "      # 'prev_word_suffix-1': '' if index == 0 else sentence[index - 1][-1:],\n",
    "        'prev_word_suffix-2': '' if index == 0 else sentence[index - 1][-2:],\n",
    "      # 'prev_word_suffix-3': '' if index == 0 else sentence[index - 1][-3:],\n",
    "      # 'prev_words': '' if index < 2 else sentence[index - 2] + ' ' + sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "      # 'next_word_suffix-1': '' if index == len(sentence) - 1 else sentence[index + 1][-1:],\n",
    "        'next_word_suffix-2': '' if index == len(sentence) - 1 else sentence[index + 1][-2:],\n",
    "      # 'next_word_suffix-3': '' if index == len(sentence) - 1 else sentence[index + 1][-3:],\n",
    "      # 'next_words': '' if index >= len(sentence) - 2 else sentence[index + 1] + ' ' + sentence[index + 2]\n",
    "    }\n",
    "\n",
    "results={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1/1.4 - own classifier on treebank/brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfomance of own model on Treebank Corpus:\n",
      "Train the model...\n",
      "Training done.\n",
      "Classify test data...\n",
      "Classification done. Accuracy: 0.8545015684790763.\n",
      "\n",
      "\n",
      "Perfomance of own model on Brown Corpus:\n",
      "Train the model...\n",
      "Training done.\n",
      "Classify test data...\n",
      "Classification done. Accuracy: 0.7558431428741575.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "treebank_tagged = nltk.corpus.treebank.tagged_sents()\n",
    "brown_tagged = brown.tagged_sents(categories='news')\n",
    "\n",
    "treebank_size=len(treebank_tagged)\n",
    "treebank_train=treebank_tagged[:(int)(treebank_size*0.8)]\n",
    "treebank_test=treebank_tagged[(int)(treebank_size*0.8):]\n",
    "\n",
    "brown_size=len(brown_tagged)\n",
    "brown_test=brown_tagged[:(int)(brown_size*0.8)]\n",
    "brown_train=brown_tagged[(int)(brown_size*0.8):]\n",
    "\n",
    "performance={}\n",
    "\n",
    "for train_data, test_data, corpus_name in [(treebank_train, treebank_test, 'Treebank Corpus'), (brown_train, brown_test[:783], 'Brown Corpus')]:\n",
    "\n",
    "    print('Perfomance of own model on ' + corpus_name + ':')\n",
    "\n",
    "    X, y = transform_to_dataset(train_data)\n",
    "\n",
    "    size=5000\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('vectorizer', DictVectorizer(sparse=False)),\n",
    "        ('classifier', GaussianNB())\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    print('Train the model...')\n",
    "    clf.fit(X[:size],y[:size])\n",
    "    print('Training done.')\n",
    "\n",
    "\n",
    "    print('Classify test data...')\n",
    "    score=0.0\n",
    "    for i in range(0,((int)(len(test_data)/100)+1)):\n",
    "        if (i+1)*100>len(test_data):\n",
    "            endval=len(test_data)\n",
    "        else:\n",
    "            endval=(i+1)*100\n",
    "\n",
    "        X_test, y_test = transform_to_dataset(test_data[i*100:endval])\n",
    "        score+=(endval-i*100)*clf.score(X_test, y_test)\n",
    "\n",
    "    score=score/len(test_data)\n",
    "    performance[corpus_name]=score\n",
    "    print('Classification done. Accuracy: ' + str(score) + '.')\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "results['own']=performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2/1.5 - NLTK tagger on treebank/brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfomance of model on Treebank Corpus:\n",
      "Classify test data with NLTK tagger...\n",
      "Classification done. Accuracy: 0.8937072708218973.\n",
      "\n",
      "\n",
      "Perfomance of model on Brown Corpus:\n",
      "Classify test data with NLTK tagger...\n",
      "Classification done. Accuracy: 0.5874052258913466.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance={}\n",
    "\n",
    "for test_data, corpus_name in [(treebank_test, 'Treebank Corpus'), (brown_test, 'Brown Corpus')]:\n",
    "\n",
    "    print('Perfomance of model on ' + corpus_name + ':')\n",
    "\n",
    "    print('Classify test data with NLTK tagger...')\n",
    "\n",
    "    y_nltk, y = [], []\n",
    "    for tagged in test_data:\n",
    "        y_nltk+=nltk.pos_tag([w for w, t in tagged])\n",
    "        y+=[t for w, t in tagged]\n",
    "\n",
    "    score=0\n",
    "    for i in range(0, len(y)):\n",
    "        if y_nltk[i][1]==y[i]:\n",
    "            score+=1\n",
    "\n",
    "    score=score/len(y)\n",
    "    performance[corpus_name]=score\n",
    "\n",
    "    print('Classification done. Accuracy: ' + str(score) + '.')\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "results['nltk']=performance\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3/1.6 - rule-based classifiers on treebank/brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfomance of model on Treebank Corpus:\n",
      "\n",
      "Performance of Default Tagger:\n",
      "on training data: 0.12729888264692388\n",
      "on test data: 0.1447677029791906\n",
      "\n",
      "Performance of Unigram Tagger:\n",
      "on training data: 0.9597455262472562\n",
      "on test data: 0.8604720794450821\n",
      "\n",
      "Performance of Bigram Tagger:\n",
      "on training data: 0.9087143618934236\n",
      "on test data: 0.11322920305404462\n",
      "\n",
      "Performance of Trigram Tagger:\n",
      "on training data: 0.9089747882485708\n",
      "on test data: 0.06706921503069016\n",
      "\n",
      "Performance of Regex Tagger:\n",
      "on training data: 0.2138472413408237\n",
      "on test data: 0.24232746145017217\n",
      "\n",
      "\n",
      "Perfomance of model on Brown Corpus:\n",
      "\n",
      "Performance of Default Tagger:\n",
      "on training data: 0.1209595837949805\n",
      "on test data: 0.1334795413246444\n",
      "\n",
      "Performance of Unigram Tagger:\n",
      "on training data: 0.9475408256659762\n",
      "on test data: 0.6978006140735635\n",
      "\n",
      "Performance of Bigram Tagger:\n",
      "on training data: 0.7955103810395491\n",
      "on test data: 0.05351212481985087\n",
      "\n",
      "Performance of Trigram Tagger:\n",
      "on training data: 0.8671901343995375\n",
      "on test data: 0.039187919042546523\n",
      "\n",
      "Performance of Regex Tagger:\n",
      "on training data: 0.19683029047641987\n",
      "on test data: 0.20493765273513379\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patterns = [(r'.*ing$', 'VBG'), (r'.*ed$', 'VBD'), (r'.*es$', 'VBZ'), (r'.*ould$', 'MD'), (r'.*\\'s$', 'NN$'),               \n",
    "             (r'.*s$', 'NNS'), (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')]\n",
    "\n",
    "performance_def=performance_uni=performance_bi=performance_tri=performance_regex={}\n",
    "for train_sents, test_sents, corpus_name in [(treebank_train, treebank_test, 'Treebank Corpus'), (brown_train, brown_test, 'Brown Corpus')]:\n",
    "\n",
    "    print('Perfomance of model on ' + corpus_name + ':')\n",
    "    print()\n",
    "    \n",
    "\n",
    "    def_model = nltk.DefaultTagger('NN')\n",
    "    uni_model = nltk.UnigramTagger(train_sents)\n",
    "    bi_model = nltk.BigramTagger(train_sents)\n",
    "    tri_model = nltk.TrigramTagger(train_sents)\n",
    "    regexp_model = nltk.RegexpTagger(patterns)\n",
    "\n",
    "    performance=def_model.evaluate(test_sents)\n",
    "    print('Performance of Default Tagger:')\n",
    "    print('on training data: ' + str(def_model.evaluate(train_sents)))\n",
    "    print('on test data: ' + str(performance))\n",
    "    performance_def[corpus_name]=performance\n",
    "    print()\n",
    "          \n",
    "    performance=uni_model.evaluate(test_sents)\n",
    "    print('Performance of Unigram Tagger:')\n",
    "    print('on training data: ' + str(uni_model.evaluate(train_sents)))\n",
    "    print('on test data: ' + str(performance))\n",
    "    performance_uni[corpus_name]=performance\n",
    "    print()\n",
    "    \n",
    "    performance=bi_model.evaluate(test_sents)\n",
    "    print('Performance of Bigram Tagger:')\n",
    "    print('on training data: ' + str(bi_model.evaluate(train_sents)))\n",
    "    print('on test data: ' + str(performance))\n",
    "    performance_bi[corpus_name]=performance\n",
    "    print()\n",
    "    \n",
    "    performance=tri_model.evaluate(test_sents)\n",
    "    print('Performance of Trigram Tagger:')\n",
    "    print('on training data: ' + str(tri_model.evaluate(train_sents)))\n",
    "    print('on test data: ' + str(performance))\n",
    "    print()\n",
    "    performance_tri[corpus_name]=performance\n",
    "    \n",
    "    performance=regexp_model.evaluate(test_sents)\n",
    "    print('Performance of Regex Tagger:')\n",
    "    print('on training data: ' + str(regexp_model.evaluate(train_sents)))\n",
    "    print('on test data: ' + str(performance))\n",
    "    performance_regex[corpus_name]=performance\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "results['def']=performance_def\n",
    "results['uni']=performance_uni\n",
    "results['bi']=performance_bi\n",
    "results['tri']=performance_tri\n",
    "results['regex']=performance_regex\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results for Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Treebank Corpus",
         "type": "bar",
         "x": [
          "own Tagger",
          "NLTK tagger",
          "Default Tagger",
          "Unigram Tagger",
          "Bigram Tagger",
          "Trigram Tagger",
          "Regex Tagger"
         ],
         "y": [
          0.8545015684790763,
          0.8937072708218973,
          0.24232746145017217,
          0.24232746145017217,
          0.24232746145017217,
          0.24232746145017217,
          0.24232746145017217
         ]
        },
        {
         "name": "Brown Corpus",
         "type": "bar",
         "x": [
          "own Tagger",
          "NLTK tagger",
          "Default Tagger",
          "Unigram Tagger",
          "Bigram Tagger",
          "Trigram Tagger",
          "Regex Tagger"
         ],
         "y": [
          0.7558431428741575,
          0.5874052258913466,
          0.20493765273513379,
          0.20493765273513379,
          0.20493765273513379,
          0.20493765273513379,
          0.20493765273513379
         ]
        }
       ],
       "layout": {
        "barmode": "group"
       }
      },
      "text/html": [
       "<div id=\"ea55d25b-8dfc-42d2-8bf3-bdc0514f52e1\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ea55d25b-8dfc-42d2-8bf3-bdc0514f52e1\", [{\"type\": \"bar\", \"x\": [\"own Tagger\", \"NLTK tagger\", \"Default Tagger\", \"Unigram Tagger\", \"Bigram Tagger\", \"Trigram Tagger\", \"Regex Tagger\"], \"name\": \"Treebank Corpus\", \"y\": [0.8545015684790763, 0.8937072708218973, 0.24232746145017217, 0.24232746145017217, 0.24232746145017217, 0.24232746145017217, 0.24232746145017217]}, {\"type\": \"bar\", \"x\": [\"own Tagger\", \"NLTK tagger\", \"Default Tagger\", \"Unigram Tagger\", \"Bigram Tagger\", \"Trigram Tagger\", \"Regex Tagger\"], \"name\": \"Brown Corpus\", \"y\": [0.7558431428741575, 0.5874052258913466, 0.20493765273513379, 0.20493765273513379, 0.20493765273513379, 0.20493765273513379, 0.20493765273513379]}], {\"barmode\": \"group\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"ea55d25b-8dfc-42d2-8bf3-bdc0514f52e1\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ea55d25b-8dfc-42d2-8bf3-bdc0514f52e1\", [{\"type\": \"bar\", \"x\": [\"own Tagger\", \"NLTK tagger\", \"Default Tagger\", \"Unigram Tagger\", \"Bigram Tagger\", \"Trigram Tagger\", \"Regex Tagger\"], \"name\": \"Treebank Corpus\", \"y\": [0.8545015684790763, 0.8937072708218973, 0.24232746145017217, 0.24232746145017217, 0.24232746145017217, 0.24232746145017217, 0.24232746145017217]}, {\"type\": \"bar\", \"x\": [\"own Tagger\", \"NLTK tagger\", \"Default Tagger\", \"Unigram Tagger\", \"Bigram Tagger\", \"Trigram Tagger\", \"Regex Tagger\"], \"name\": \"Brown Corpus\", \"y\": [0.7558431428741575, 0.5874052258913466, 0.20493765273513379, 0.20493765273513379, 0.20493765273513379, 0.20493765273513379, 0.20493765273513379]}], {\"barmode\": \"group\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_notebook_mode(connected=True)\n",
    "\n",
    "x1='Treebank Corpus'\n",
    "x2='Brown Corpus'\n",
    "trace1 = go.Bar(\n",
    "    x=['own Tagger', 'NLTK tagger', 'Default Tagger', 'Unigram Tagger', 'Bigram Tagger', 'Trigram Tagger', 'Regex Tagger'],\n",
    "    y=[results['own'][x1], results['nltk'][x1], results['def'][x1], results['uni'][x1], results['bi'][x1], results['tri'][x1], results['regex'][x1]],\n",
    "    name=x1\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=['own Tagger', 'NLTK tagger', 'Default Tagger', 'Unigram Tagger', 'Bigram Tagger', 'Trigram Tagger', 'Regex Tagger'],\n",
    "    y=[results['own'][x2], results['nltk'][x2], results['def'][x2], results['uni'][x2], results['bi'][x2], results['tri'][x2], results['regex'][x2]],\n",
    "    name=x2\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='grouped-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - importing a Spanish corpus \n",
    "\n",
    "The IULA Corpus needs to be imported using the ConllCorpusReader. Since the format of the POS tags is different from those assigned by the RDRPOSTagger used in Task 2.2, the POS tags will be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = nltk.corpus.ConllCorpusReader('RDRPOSTagger-master/data/es/', ['IULA_Spanish_LSP_Treebank.conll'], ['ignore', 'words', 'ignore', 'pos', 'ignore', 'ignore', 'ignore', 'ignore', 'ignore', 'ignore'])\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "file=open('RDRPOSTagger-master/data/es/IULA_Corpus', 'w')\n",
    "for sent in corp.sents():\n",
    "    for word in sent:\n",
    "        file.write(word + ' ')\n",
    "    file.write('\\n')\n",
    "iula_sents=corp.tagged_sents()\n",
    "\n",
    "IULA_tagged_words=[]\n",
    "tagged_words=corp.tagged_words()\n",
    "for i in range(0, len(tagged_words)):\n",
    "    tag=tagged_words[i][1]\n",
    "    if tag in ['n', 'i', '_']:\n",
    "        new_tag='NOUN'\n",
    "    elif tag=='v':\n",
    "        new_tag='VERB'\n",
    "    elif tag=='a':\n",
    "        new_tag='ADJ'\n",
    "    elif tag=='d':\n",
    "        new_tag='DET'\n",
    "    elif tag=='c':\n",
    "        new_tag='CONJ'\n",
    "    elif tag=='s':\n",
    "        new_tag='ADP'\n",
    "    elif tag=='f':\n",
    "        new_tag='PUNCT'\n",
    "    elif tag in ['z', 'w']:\n",
    "        new_tag='NUM'\n",
    "    elif tag=='p':\n",
    "        new_tag='PRON'\n",
    "    elif tag=='r':\n",
    "        new_tag='ADV'\n",
    "    else:\n",
    "        new_tag=tag\n",
    "    IULA_tagged_words.append((tagged_words[i][0], new_tag))\n",
    "\n",
    "iula_size=len(iula_sents)\n",
    "iula_train=iula_sents[:(int)(0.8*iula_size)]\n",
    "iula_test=iula_sents[(int)(0.8*iula_size):]\n",
    "\n",
    "results={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1 own POS tagger model on IULA Corpus\n",
    "\n",
    "I played around with common suffixes in the feature model for the spanish corpus. To my surprise the model performed better without the added features. I did not check all different configurations but of those I tried the original feature model performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfomance of own model on IULA Corpus:\n",
      "Train the model...\n",
      "Training done.\n",
      "Classify test data...\n",
      "Classification done. Accuracy: 0.8217944417247102.\n"
     ]
    }
   ],
   "source": [
    "print('Perfomance of own model on IULA Corpus:')\n",
    "\n",
    "size=5000\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "def feat_span(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index==0,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'stem': '',\n",
    "        'suffix': '',\n",
    "        'prefix-1': sentence[index][:1],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "      # 'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1:],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'suffix-4': sentence[index][-5:],\n",
    "      # 'is_all_caps': sentence[index] == sentence[index].upper(),\n",
    "      # 'contains_number': len([ch for ch in sentence[index] if ch.isdigit()])>0,\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "      # 'prev_stem': '',\n",
    "      #  'prev_suffix': '',\n",
    "      # 'prev_word_suffix-1': '' if index == 0 else sentence[index - 1][-1:],\n",
    "        'prev_word_suffix-2': '' if index == 0 else sentence[index - 1][-2:],\n",
    "      # 'prev_word_suffix-3': '' if index == 0 else sentence[index - 1][-3:],\n",
    "      # 'prev_words': '' if index < 2 else sentence[index - 2] + ' ' + sentence[index - 1],\n",
    "      # 'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "      # 'next_stem': '',\n",
    "      # 'next_suffix': '',\n",
    "      # 'next_word_suffix-1': '' if index == len(sentence) - 1 else sentence[index + 1][-1:],\n",
    "        'next_word_suffix-2': '' if index == len(sentence) - 1 else sentence[index + 1][-2:],\n",
    "      # 'next_word_suffix-3': '' if index == len(sentence) - 1 else sentence[index + 1][-3:],\n",
    "      # 'next_words': '' if index >= len(sentence) - 2 else sentence[index + 1] + ' ' + sentence[index + 2]\n",
    "    }\n",
    "\n",
    "comm_suffixes={'@', 'a', 'aba', 'abais', 'ábamos', 'aban', 'abas', 'able', 'ácea', 'áceo', 'acha', 'acho', 'ación', 'ada',\n",
    "               'adgo', 'ado', 'ador', 'adora', 'adura', 'áis', 'aje', 'ajo', 'al', 'algia', ' amento', 'amiento', 'amos',\n",
    "               'an', 'ana', 'ancia', 'ando', 'ano', 'ante', 'anza', 'ar', 'aran', 'arca', 'areis', 'ario', 'aron', 'aré',\n",
    "               'as', 'asa', 'astro', 'ata', 'ato', 'avo', 'aza', 'azgo', 'azo', 'bilidad', 'ceta', 'cete', 'cida', 'cidio',\n",
    "               'cigótico', 'cilla', 'cillo', 'ción', 'cita', 'cito', 'clasa', 'cola', 'cracia', 'crata', 'dad', 'dero',\n",
    "               'dor', 'dora', 'dura', 'ear', 'ececilla', 'ececillo', 'ececita', 'ececito', 'ecer', 'ecilla', 'ecillo',\n",
    "               'ecita', 'ecito', 'ectomía', 'eda', 'edad', 'edo', 'edor', 'edora', 'edura', 'éis', 'ejo', 'emos', 'en',\n",
    "               'eña', 'eno', 'eño', 'ense', 'ente', 'eo', 'er', 'era', 'ería', 'ero', 'eré', 'eréis', 'es', 'és', 'esa',\n",
    "               'esca', 'esco', 'eta', 'ete', 'ez', 'eza', 'ezna', 'ezno', 'faga', 'fago', 'fila', 'filia', 'filo', 'fito',\n",
    "               'fobia', 'fobo', 'fono', 'forme', 'geno', 'grafía', 'grafo', 'grama', 'génesis', 'í', 'ia', 'ía', 'íais',\n",
    "               'íamos', 'ían', 'iano', 'ías', 'iatra', ' iatría', 'ible', 'ichuela', 'ico', 'idad', 'ido', 'idor',\n",
    "               'idora', 'idura', 'iego', 'iendo', 'iente', 'ieron', 'ificar', 'il', 'illa', 'illo', 'ilo', 'imento',\n",
    "               'imiento', 'imos', 'ín', 'ina', 'ing', 'ino', 'io', 'ió', 'ío', 'iré', 'iréis', 'ísima', 'ísimas',\n",
    "               'ísimo', 'ísimos', 'ismo', 'ista', 'iste', 'isteis', 'ita', 'itis', 'ito', 'itud', 'ivo', 'iza',\n",
    "               'ización', 'izar', 'izo', 'landia', 'latría', 'lita', 'lito', 'loga', 'logía', 'lógico', 'logo',\n",
    "               'mana', 'mancia', 'mancía', 'manía', 'mano', 'mente', 'mento', 'metría', 'metro', 'miento', 'morfa',\n",
    "               'morfo', 'nauta', 'nte', 'o', 'ó', 'oico', 'oide', 'oma', 'on', 'ón', 'ona', 'onas', 'ónimo', 'or',\n",
    "               'osa', 'oso', 'ota', 'ote', 'pata', 'patía', 'plastia', 'podo', 's', 'saurio', 'sca', 'sco', 'scopia',\n",
    "               'scopía', 'scópico', 'scopio', 'teca', 'tecnia', 'terapia', 'toma', 'tomía', 'tomo', 'trofa', 'trofia',\n",
    "               'trofo', 'ucha', 'ucho', 'uco', 'udo', 'uela', 'uelo', 'ura', 'uro', 'usco', 'xión', 'yendo', 'zón',\n",
    "               'zoo', 'zuela'}\n",
    "\n",
    "\n",
    "def assign_suffixes(X):\n",
    "    for i in range(0, len(X)):\n",
    "        for suf in comm_suffixes:\n",
    "            if X[i]['word'][-len(suf):].lower()==suf:\n",
    "                X[i]['suffix']=suf\n",
    "        X[i]['stem']=X[i]['word'][:-len(X[i]['suffix'])].lower()\n",
    "        if not X[i]['is_first']:\n",
    "                X[i]['prev_stem']=X[i-1]['stem']\n",
    "                X[i]['prev_suffix']=X[i-1]['suffix']\n",
    "                X[i-1]['next_stem']=X[i]['stem']\n",
    "                X[i-1]['next_suffix']=X[i]['suffix']\n",
    "        return X\n",
    "            \n",
    "def transform_to_dataset_span(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(feat_span(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    "    \n",
    "    return assign_suffixes(X), y\n",
    "\n",
    "\n",
    "X, y = transform_to_dataset_span(iula_train)\n",
    "\n",
    "print('Train the model...')\n",
    "clf.fit(X[:size],y[:size])\n",
    "print('Training done.')\n",
    "\n",
    "\n",
    "print('Classify test data...')\n",
    "score=0.0\n",
    "for i in range(0,((int)(len(iula_test)/100)+1)):\n",
    "    if (i+1)*100>len(iula_test):\n",
    "        endval=len(iula_test)\n",
    "    else:\n",
    "        endval=(i+1)*100\n",
    "\n",
    "    X_test, y_test = transform_to_dataset_span(iula_test[i*100:endval])\n",
    "\n",
    "            \n",
    "    score+=(endval-i*100)*clf.score(X_test, y_test)\n",
    "\n",
    "score=score/len(iula_test)\n",
    "print('Classification done. Accuracy: ' + str(score) + '.')\n",
    "\n",
    "results['own']=score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Task 2.2 - RDRPOSTagger on IULA Corpus\n",
    "\n",
    "The subprocess call for the RDRPOSTagger is commented out since it requires root privileges and therefore should not be executed in Jupyter. It is just there for demonstration purposes and was executed in a separate shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfomance of RDR POS Tagger on IULA Corpus:\n",
      "Classify test data...\n",
      "Classification done. Accuracy: 0.931034424211554.\n"
     ]
    }
   ],
   "source": [
    "print('Perfomance of RDR POS Tagger on IULA Corpus:')\n",
    "\n",
    "\n",
    "'''\n",
    "arg1='RDRPOSTagger-master/data/es/es-upos.RDR'\n",
    "arg2='RDRPOSTagger-master/data/es/es-upos.DICT'\n",
    "arg3='RDRPOSTagger-master/data/es/IULA_Corpus'\n",
    "subprocess.call(['RDRPOSTagger-master/pSCRDRtagger/RDRPOSTagger.py', arg1, arg2, arg3])\n",
    "'''\n",
    "file=open('RDRPOSTagger-master/data/es/IULA_Corpus.TAGGED', 'r')\n",
    "\n",
    "RDRPOS_res=[]\n",
    "for line in file:\n",
    "    sent=line.split(' ')\n",
    "    for tagged_word in sent:\n",
    "        it=tagged_word.rfind('/')\n",
    "        word=tagged_word[:it]\n",
    "        tag=tagged_word[it+1:]\n",
    "        if tag[-1:]=='\\n':\n",
    "            tag=tag[:-1]\n",
    "        if tag in ['PROPN', 'X']:\n",
    "            tag='NOUN'\n",
    "        if tag=='AUX':\n",
    "            tag='VERB'\n",
    "        if tag in ['CCONJ', 'SCONJ']:\n",
    "            tag='CONJ'\n",
    "        if tag=='SYM':\n",
    "            tag='ADP'\n",
    "        RDRPOS_res.append((word, tag))\n",
    "        \n",
    "print('Classify test data...')\n",
    "score=0\n",
    "for i in range(0,len(RDRPOS_res)):\n",
    "    if RDRPOS_res[i][1]==IULA_tagged_words[i][1]:\n",
    "        score+=1\n",
    "score=score/len(RDRPOS_res)\n",
    "print('Classification done. Accuracy: ' + str(score) + '.')\n",
    "results['rdr']=score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "own Tagger",
          "RDR POS Tagger"
         ],
         "y": [
          0.8217944417247102,
          0.931034424211554
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"9a95ed0b-f4a1-4d33-8b79-33999feef823\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9a95ed0b-f4a1-4d33-8b79-33999feef823\", [{\"type\": \"bar\", \"x\": [\"own Tagger\", \"RDR POS Tagger\"], \"y\": [0.8217944417247102, 0.931034424211554]}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9a95ed0b-f4a1-4d33-8b79-33999feef823\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9a95ed0b-f4a1-4d33-8b79-33999feef823\", [{\"type\": \"bar\", \"x\": [\"own Tagger\", \"RDR POS Tagger\"], \"y\": [0.8217944417247102, 0.931034424211554]}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [go.Bar(\n",
    "            x=['own Tagger', 'RDR POS Tagger'],\n",
    "            y=[results['own'], results['rdr']]\n",
    "    )]\n",
    "\n",
    "iplot(data, filename='basic-bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
